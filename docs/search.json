[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The CLIP Chronicles",
    "section": "",
    "text": "Contrastive Language-Image Pre-training (CLIP) is a Convolutional Neural Network (CNN) that learns visual concepts from natural language supervision and has been trained on a combination of images and their captions. CLIP bridges the gap between traditional CNN and natural language processing, providing several innovations and advancements such as generalization across tasks, increased flexibility, and scalability, and having an interface to natural language.\nAs part of Deep Learning for Data Science (IDC 6146) at the University of West Florida, our team critically analyzed, assessed, and reproduced the CLIP model and elaborated on potential applications in a professional report. This website was built using R/Quarto while the code for the analysis was built using Python/Jupyter, representing an interdisciplinary and collaborative approach to this group project.\nThis project was completed under the guidance of Dr. Shusen Pu (Shusen Pu | Dr. Shusen Pu).\n\nFeel free to meet our team, on the About tab and review our report on the Report tab. Our raw code is on the Code tab with code and slides extracted for general use."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Us",
    "section": "",
    "text": "This is the directory for the IDC 6146 group: The CLIP Chronicles. Thank you for stopping by our page!\n\nChinmay Shah, M.S.\nContributions: Worked on writing the code and replication of the results section to train the model on a subset of data. Additionally worked on writing the Methodology section and the replication of the results part of the report and formatting of the report.\nContact Chinmay\n\n\nJoshua J. Cook, M.S., ACRP-PM, CCRC\nContributions: Provided the project outline based on previous papers and literature review. Wrote the introduction/related work section. Developed  initial code and accuracy data for implementing and training CLIP on the initial dataset. Contributed to potential applications of CLIP, with a focus on healthcare/medicine. Developed the project website.\nContact Josh\n\n\nPraya Cheekapara, B.S.\nContributions: Impact and Critical Assessment(analysis), Replication and Extension of Results.Worked on written portion of this section in the paper and classification examples (4.3).\nContact Praya\n\n\nMoise Brutus, B.S.\nContributions: Worked on the limitation and conclusion sections. Was responsible for finding and writing a script for the dataset section. Created the Discord server to facilitate group communication/group meetings.\nContact Moise\n\n\nShusen Pu, Ph.D.\nContributions: Professor and mentor throughout the course and project.\nContact Dr. Pu"
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "Here’s a copy of the final code we used to train and test our model (attached as a Jupyter notebook).\nDownload Jupyter Notebook"
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "Here’s a copy of the final slideshow we presented as part of the group project to Dr. Pu on March 1st, 2024.\nDownload Slides"
  },
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "Report",
    "section": "",
    "text": "Inception of one of the first artificial neural networks is credited to Dr. Kunihiko Fukushima in the 1980s. In his landmark paper, Fukushima builds on earlier work by Hubel and Wiesel who conceptualized a network of simple and complex “cells” that are capable of recognizing stimulus patterns based on geometrical similarity that simulated the way the human visual cortex processes visual information (Fukushima 1980). This network was nicknamed the “Neocognitron,” and was built off of Fukushima’s original concept of the “Cognitron’’ in 1975, which was also a self-organizing multilayered neural network model (Fukushima 1980). The main improvement introduced in the Neocognitron was the response of the network not being affected by the position of individual stimulus patterns (i.e., shifted patterns) (Fukushima 1980). Additionally, the Neocognitron introduced architecture capable of supervised learning in addition to the unsupervised methods that were present in the Cognitron (Fukushima 1980). Both the Cognitron and the Neocognitron laid the foundation for the development of more advanced convolutional neural networks (CNNs) throughout the 1990s and to modern models, as shown in Figure 1. \n\nFigure 1. Timeline of CNN development. CNN development was initiated by the work of Fukushima in the 1970s and 1980s via the introduction of the Cognitron and Neocognitron. This spurred the development of more complex CNNs during the 1990s such as LeNet-5, and modern models such as GoogLeNet and CLIP (Fukushima 1980; Krizhevsky et al. 2012; Lecun et al. 1998; Sabour et al. 2017; Simonyan and Zisserman 2014; Szegedy et al. 2015)\nIn the late 1990s, LeCun Et. Al. applied the concept of backpropagation to neural networks with the goal of recognizing digits of different handwriting styles (Lecun et al. 1998). Backpropagation is a gradient estimation algorithm used to train neural networks, including CNNs. In their paper, LeCun Et. Al. utilized the MNIST database that contained 60,000 training and 10,000 testing images of handwriting from the American Census Bureau (Lecun et al. 1998). LeCun Et. Al. directly built on the work of Fukushima, creating the LeNet CNN by giving the model an example image, asking it to predict the image (i.e., training), and then updating the model setting by comparing the ground truth to the predicted label (i.e., testing) (Lecun et al. 1998). The LeNet model (now known as LeNet-5) was a groundbreaking application of artificial neural networks from which further development in the 2000s and 2010s was built on (Lecun et al. 1998). \nOne of the primary limitations of developing models like LeCun-5 stemmed from the availability of large-scale image data. Due to this, in 2006 Fei-Fei Li began working on a database of image data that could be widely utilized by artificial intelligence researchers. This database became ImageNet, which currently houses over 14 million images that are organized according to WordNet hierarchy. ImageNet is available to researchers, but is also the focus of the annual ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which evaluates algorithms for object detection and image classification. The introduction of ImageNet did two things: first, a large image-based dataset became widely available for researchers, and two, an annual competition was created that encouraged innovations in CNN development. \nIn 2012, the third year of the ILSVRC, Alex Krizhevsky Et. Al. won the competition with a top-5 error of 15.3%, which was 10.8% lower than the runner-up and utilizes a CNN (Krizhevsky et al. 2012). The network, now known as AlexNet, has 8 layers (5 convolutional and 3 fully-connected) that were trained on the ImageNet dataset (which included 1.2 million high-resolution images at the time) (Krizhevsky et al. 2012). AlexNet represented a huge leap forward in the complexity of CNNs, with 60 million parameters and 650,000 neurons (Krizhevsky et al. 2012). Building on this, the 2014 and 2015 ILSVRC were won by Karen Simonyan Et. Al. for their development of VGGNet and Google for their development of GoogleLeNet respectively (Simonyan and Zisserman 2014; Szegedy et al. 2015). Both of these implementations of CNN were focused at increasing the depth of the networks (Simonyan and Zisserman 2014; Szegedy et al. 2015). Microsoft introduced ResNet in 2015. \nMore recently, in 2017, Geoffrey Hinton Et. Al. introduced the idea of Capsule Networks, which are groups of neurons in a network (capsules) that can detect objects or parts of objects across different viewpoints (Sabour et al. 2017). This innovation provided several improvements on traditional CNNs, including the preservation of spatial hierarchies, generalization to new views, and reduced data requirements(Sabour et al. 2017). A few years later, in 2019, Google introduced EfficientNet, which can scale CNNs in a very structured measure and provides high accuracy with fewer parameters using the ImageNet database. Most recently in 2021, Radford Et. Al. at OpenAI introduced the contrastive language-image pre-training (CLIP) (Radford et al. 2021). CLIP is a CNN that learns visual concepts from natural language supervision, and has been trained on a combination of images and their captions (Radford et al. 2021) CLIP bridges the gap between traditional CNN and natural language processing, providing several innovations and advancements such as generalization across tasks, increased flexibility and scalability, and having an interface to natural language (Radford et al. 2021)."
  },
  {
    "objectID": "literature.html",
    "href": "literature.html",
    "title": "Literature",
    "section": "",
    "text": "What is the goal of the paper?\nThe paper presents the lme4 package for R, which facilitates the fitting of linear mixed-effects models. The authors aim to articulate the package’s capabilities in evaluating the profiled deviance or REML criterion for linear mixed models and to explain the representation and optimization of such models for parameter estimation.\nWhy is it important?\nThe significance of this paper lies in its contribution to the field of computational methods for fitting mixed models—an area with many open problems. The lme4 package represents an evolution in this domain, offering more efficient computational tools and a syntax that simplifies the modeling process, especially for models with crossed random effects.\nHow is it solved? – methods\nThe package utilizes maximum likelihood or restricted maximum likelihood (REML) estimates for linear mixed-effects model parameters, employing numerical representation and optimization functions within R. The paper delves into the model’s structure, the evaluative steps for the profiled deviance or REML criterion, and the class structure representing such models, highlighting the improvements over previous formulations.\nResults/limitations, if any.\nThe document focuses more on methodology than specific results or limitations. It details the improvement over the nlme package, addressing efficient linear algebra tools and the incorporation of profile likelihood confidence intervals on random-effects parameters. The paper emphasizes the ongoing development of the lme4 package, acknowledging the need for stability and usability for a broad range of applications.\n\n\n\nWhat is the goal of the paper?\nThe goal of the paper is to provide a detailed introduction to developing and interpreting linear mixed-effects models for repeated measurements in the context of cardiothoracic surgery outcomes research. The paper uses a dataset on patients undergoing surgical pulmonary valve replacement to illustrate the steps of developing such models for clinician researchers.\nWhy is it important?\nThis work is important because the emergence of large cardio-thoracic surgery datasets, including repeated measurements over time, presents an opportunity to apply advanced modeling of outcomes. Linear mixed-effects models offer a more nuanced understanding of these outcomes compared to traditional methods, which is crucial for enhancing clinical decision-making and patient care.\nHow is it solved? – methods\nThe authors used a retrospective dataset containing serial echocardiographic measurements from patients who underwent surgical pulmonary valve replacement at Erasmus MC between 1986 and 2017. The paper discusses the construction of the model, including dealing with missing values, correlated variables, and multicollinearity. It also covers model specification, variable selection, addressing nonlinearity, and interpretation of results. An R script is provided for implementing the model.\nResults/limitations, if any.\nThe paper illustrates the construction of the model, including essential aspects such as theories of linear mixed-effects models, missing values, collinearity, interaction, nonlinearity, model specification, and results interpretation. It shows that linear mixed-effects models provide a more detailed view of repeated measurements and give more valid estimates compared to linear regression models, especially in the context of cardio-thoracic surgery outcomes research. Limitations related to model assumptions, such as linearity and normal distribution of residuals, are addressed through transformations and statistical tests.\n\n\n\nWhat is the goal of the paper?\nThe goal of this paper is to provide a comprehensive guide on the use and application of Generalized Linear Mixed Models (GLMMs) for ecologists and evolutionary biologists dealing with nonnormal data types, such as counts or proportions, which often do not fit well with classical statistical procedures. The paper aims to clarify the use of GLMMs, given the popularity of these models in recent years.\nWhy is it important?\nThe importance of this paper lies in its attempt to introduce GLMMs for biologists, where data often fall outside the scope of methods taught in introductory statistics classes. The paper highlights the limitations of traditional shortcuts like data transformation or ignoring random effects and advocates for GLMMs as a more appropriate statistical approach for nonnormal data with random effects.\nHow is it solved? – methods\nThe paper reviews the use and misuse of GLMMs in biology, discusses estimation and inference, and summarizes best-practice data analysis procedures. It emphasizes the need for researchers to match their statistical approaches to their data, rather than forcing data into classical statistical frameworks. The paper discusses various estimation algorithms for fitting GLMMs, including maximum likelihood (ML), pseudo- and penalized quasilikelihood (PQL), Laplace approximations, Gauss-Hermite quadrature (GHQ), and Markov chain Monte Carlo (MCMC) algorithms.\nResults/limitations, if any.\nWhile the paper provides a broad overview of GLMM procedures and best practices, it also acknowledges the challenges and controversies in statistical issues such as null hypothesis testing, stepwise regression, and the use of Bayesian statistics. It highlights that GLMMs are powerful tools but can be challenging to use, even for statisticians, due to computational difficulties in estimating parameters, especially for complex models or large numbers of random effects.\n\n\n\nWhat is the goal of the paper?\nThe goal of this paper is to introduce linear mixed-effects models (LMMs) as a versatile tool for analyzing data from within-participant psychology experiments. It seeks to address the limitations of traditional analysis methods like ANOVA in handling complex data structures, such as those involving repeated measures or nested designs. The paper also introduces LMMgui, a free, graphical user interface designed to facilitate the use of LMMs for researchers using R.\nWhy is it important?\nThe importance of this work lies in its potential to enhance the analysis of experimental psychology data by providing a more flexible and robust statistical tool that can handle the complexities of within-participant designs, such as pseudoreplication and missing data. By offering a user-friendly interface for LMM analysis, the paper aims to make advanced statistical methods more accessible to researchers, thereby improving the quality and interpretability of psychological research.\nHow is it solved? – methods\nThe paper discusses the theoretical foundation of LMMs, explaining how they can accommodate various data structures and assumptions that are commonly encountered in psychology experiments. It contrasts LMMs with traditional repeated-measures ANOVA, highlighting the advantages of LMMs in terms of their flexibility and fewer stringent assumptions. The introduction of LMMgui is a significant methodological contribution, providing a step-by-step guide on how to use this tool to specify and compare different LMMs for data analysis.\nResults/limitations, if any.\nWhile the paper primarily serves as a tutorial and does not present results from a specific study, it effectively demonstrates the application of LMMs through hypothetical examples. These examples illustrate how LMMs can be used to analyze data from within-participant designs, accounting for random effects and complex variance-covariance structures. The paper acknowledges the challenges in interpreting LMM results and the potential for increased Type I error rates in certain conditions, emphasizing the need for careful model comparison and validation.\n\n\n\nWhat is the goal of the paper?\nThe goal of this paper is to estimate baseline mortality (mortality under non-pandemic conditions for Belgium and the Netherlands using a linear mixed model (LMM), which can account for both fixed and random effects. If baseline mortality can be modeled, then excess mortality (the measure of the increase in mortality from all causes during a specific time period) can be used to evaluate the impact of COVID-19 on mortality.\nWhy is it important?\nHistorically, 5-year weekly averages have been used to determine baseline mortality. However, this excludes year-specific trends in mortality and the effects of historical excess mortality (ex: past influenza breakouts or heat waves). Using a LMM is important because it allows for more accurate modeling that accounts for these factors in the form of random effects.\nHow is it solved? – methods\nThe paper proposes a general linear mixed model to model weekly mortality as Ytj, with t = 1,…,52 weeks and by year j = 2009…,2020.\nThe model is then adjusted to: model the cyclic pattern from year to year via random effects of Fourier terms, and reduce the influence of historical excess mortality (as mentioned above) by downweighing the residuals.\nResults/limitations, if any.\nSeveral statistics were used to evaluate the model’s forecasting accuracy, including the likelihood ratio test (LRT) and the root mean square error % (RMSE%). The models were fitted to historical mortality year from 2009- week 10 of 2020. The remaining 42 weeks of 2020 were forecasted using the LMM, along with the 5-year average model, and the ground truth data. The models all performed well, so an overall recommendation to include the down-weight procedure for past excess mortality and to include a serial correlation structure were made. The LMM did fit the mortality data better and two years were better predicted compared to the 5-year weekly average models. Many limitations exist, including differences in the reporting of COVID-19 deaths in Belgium and the Netherlands, and across the world. Additionally, it is unclear if the added complexity of LMMs provide a significant benefit over 5-year weekly average models in years besides 2014 and 2016.\n\n\n\nWhat is the goal of the paper?\nThe goal of this paper is to address a potential shortcoming of LMMs, which is when data is not missing at random, but instead, data is missing that is dependent on the health-related quality-of-life of the patient (i.e., data is missing because quality-of-life – the outcome - decreased). Viewing missing data like this, a survival model may be more appropriate. Or, as this paper suggests, a joint model (JM) that includes both LMM and survival sub-models.\nWhy is it important?\nThis concept is important because clinical trials are frequently employing LMMs to evaluate longitudinal data. Clinical trials involve human subjects, which are known to be more variable compared to benchtop studies. This includes variation in the completeness of patient reported outcome measures (PROMs), especially for longitudinal studies. Many times, this variation is random, and independent of the outcome measure. However, if the outcome measure itself is having an effect on the completeness, then it could represent a major bias limitation of LMMs that should be addressed using the proposed joint models.\nHow is it solved? – methods\nThis paper first introduces the LMM that is traditional in longitudinal clinical studies like this. Then, to account for the fact that observations in quality-of-life scores ends with a dropout event, a joint model is created by linking the LMM to a survival model with shared parameters.\nThis JM was then evaluated using historical clinical trial data where a standard LMM was utilized, and in several simulation studies where extreme examples of this dependence was introduced.\nResults/limitations, if any.\nThis paper showed that poor quality-of-life scores are associated with drop out. Therefore, LMMs should be avoided when analyzing this data in clinical trials. The LMMs in both the historical studies and in the simulations were overly optimistic in estimating the quality-of-life scores. Specifically, the LMM overestimates the slope governing the prediction trajectory in both treatment and control arms. The LMM is also more optimistic for the control arm than the experimental arm, despite the protective effect of treatment on the quality-of-life score."
  },
  {
    "objectID": "literature.html#an-introduction-to-linear-mixed-effects-modeling-in-r---violet-a.-brown-2021-uwf.edu",
    "href": "literature.html#an-introduction-to-linear-mixed-effects-modeling-in-r---violet-a.-brown-2021-uwf.edu",
    "title": "Literature",
    "section": "",
    "text": "What is the goal of the paper?\nThe article aims to introduce the utility of linear mixed-effects models (LMMs) over traditional ANOVA or regression methods, emphasizing their applicability in handling intra-subject differences and their flexibility with missing values and outliers. This was specifically done in R.\nWhy is it important?\nLMMs are highlighted for their ability to not assume independence of observations, making them particularly suitable for psychological research where within-subject differences are common.\nHow is it solved? – methods\nThe article employs a psychology-based dataset to demonstrate the implementation of LMMs using R. It focuses on explaining the concepts of fixed effects and random effects in LMMs, providing the data and code for readers to follow along without delving deeply into the mathematical foundations of LMMs.\nResults/limitations, if any.\nThe primary contribution is the detailed walkthrough of LME model syntax in R and interpretation within the context of psychological data. The article suggests that the methods described should be generalizable across various industries, although it is based on a specific psychology dataset."
  },
  {
    "objectID": "literature.html#a-brief-introduction-to-mixed-effects-modelling-and-multi-model-inference-in-ecology---pmc-nih.gov",
    "href": "literature.html#a-brief-introduction-to-mixed-effects-modelling-and-multi-model-inference-in-ecology---pmc-nih.gov",
    "title": "Literature",
    "section": "",
    "text": "What is the goal of the paper?\nThis article aims to demonstrate the application of LMMs and multi-model inference within the field of ecology, focusing on error and model selection using biological data.\nWhy is it important?\nThe significance of the article lies in its broad overview of LMMs in the context of ecological data, addressing the challenges of error and model selection in ecological research.\nHow is it solved? – methods\nThe methodology includes a detailed examination of information theory and multi-model inference, applied to example ecological data. The article provides data and code for replication and further exploration.\nResults/limitations, if any.\nWhile the article offers valuable insights into the use of LMMs in ecology, including data and code for practical application, it primarily serves as an introductory piece, potentially leaving out more advanced aspects of LMMs and multi-model inference."
  },
  {
    "objectID": "literature.html#generalized-linear-mixed-models-a-practical-guide-for-ecology-and-evolution---sciencedirect",
    "href": "literature.html#generalized-linear-mixed-models-a-practical-guide-for-ecology-and-evolution---sciencedirect",
    "title": "Literature",
    "section": "",
    "text": "What is the goal of the paper?\nThe goal of this paper is to provide a comprehensive guide on the use and application of Generalized Linear Mixed Models (GLMMs) for ecologists and evolutionary biologists dealing with nonnormal data types, such as counts or proportions, which often do not fit well with classical statistical procedures. The paper aims to clarify the use of GLMMs, given the popularity of these models in recent years.\nWhy is it important?\nThe importance of this paper lies in its attempt to introduce GLMMs for biologists, where data often fall outside the scope of methods taught in introductory statistics classes. The paper highlights the limitations of traditional shortcuts like data transformation or ignoring random effects and advocates for GLMMs as a more appropriate statistical approach for nonnormal data with random effects.\nHow is it solved? – methods\nThe paper reviews the use and misuse of GLMMs in biology, discusses estimation and inference, and summarizes best-practice data analysis procedures. It emphasizes the need for researchers to match their statistical approaches to their data, rather than forcing data into classical statistical frameworks. The paper discusses various estimation algorithms for fitting GLMMs, including maximum likelihood (ML), pseudo- and penalized quasilikelihood (PQL), Laplace approximations, Gauss-Hermite quadrature (GHQ), and Markov chain Monte Carlo (MCMC) algorithms.\nResults/limitations, if any.\nWhile the paper provides a broad overview of GLMM procedures and best practices, it also acknowledges the challenges and controversies in statistical issues such as null hypothesis testing, stepwise regression, and the use of Bayesian statistics. It highlights that GLMMs are powerful tools but can be challenging to use, even for statisticians, due to computational difficulties in estimating parameters, especially for complex models or large numbers of random effects."
  },
  {
    "objectID": "literature.html#frontiers-linear-mixed-effects-models-for-within-participant-psychology-experiments-an-introductory-tutorial-and-free-graphical-user-interface-lmmgui-frontiersin.org",
    "href": "literature.html#frontiers-linear-mixed-effects-models-for-within-participant-psychology-experiments-an-introductory-tutorial-and-free-graphical-user-interface-lmmgui-frontiersin.org",
    "title": "Literature",
    "section": "",
    "text": "What is the goal of the paper?\nThe goal of this paper is to introduce linear mixed-effects models (LMMs) as a versatile tool for analyzing data from within-participant psychology experiments. It seeks to address the limitations of traditional analysis methods like ANOVA in handling complex data structures, such as those involving repeated measures or nested designs. The paper also introduces LMMgui, a free, graphical user interface designed to facilitate the use of LMMs for researchers using R.\nWhy is it important?\nThe importance of this work lies in its potential to enhance the analysis of experimental psychology data by providing a more flexible and robust statistical tool that can handle the complexities of within-participant designs, such as pseudoreplication and missing data. By offering a user-friendly interface for LMM analysis, the paper aims to make advanced statistical methods more accessible to researchers, thereby improving the quality and interpretability of psychological research.\nHow is it solved? – methods\nThe paper discusses the theoretical foundation of LMMs, explaining how they can accommodate various data structures and assumptions that are commonly encountered in psychology experiments. It contrasts LMMs with traditional repeated-measures ANOVA, highlighting the advantages of LMMs in terms of their flexibility and fewer stringent assumptions. The introduction of LMMgui is a significant methodological contribution, providing a step-by-step guide on how to use this tool to specify and compare different LMMs for data analysis.\nResults/limitations, if any.\nWhile the paper primarily serves as a tutorial and does not present results from a specific study, it effectively demonstrates the application of LMMs through hypothetical examples. These examples illustrate how LMMs can be used to analyze data from within-participant designs, accounting for random effects and complex variance-covariance structures. The paper acknowledges the challenges in interpreting LMM results and the potential for increased Type I error rates in certain conditions, emphasizing the need for careful model comparison and validation."
  },
  {
    "objectID": "literature.html#a-linear-mixed-model-to-estimate-covid-19-induced-excess-mortality---pubmed-nih.gov",
    "href": "literature.html#a-linear-mixed-model-to-estimate-covid-19-induced-excess-mortality---pubmed-nih.gov",
    "title": "Literature",
    "section": "",
    "text": "What is the goal of the paper?\nThe goal of this paper is to estimate baseline mortality (mortality under non-pandemic conditions for Belgium and the Netherlands using a linear mixed model (LMM), which can account for both fixed and random effects. If baseline mortality can be modeled, then excess mortality (the measure of the increase in mortality from all causes during a specific time period) can be used to evaluate the impact of COVID-19 on mortality.\nWhy is it important?\nHistorically, 5-year weekly averages have been used to determine baseline mortality. However, this excludes year-specific trends in mortality and the effects of historical excess mortality (ex: past influenza breakouts or heat waves). Using a LMM is important because it allows for more accurate modeling that accounts for these factors in the form of random effects.\nHow is it solved? – methods\nThe paper proposes a general linear mixed model to model weekly mortality as Ytj, with t = 1,…,52 weeks and by year j = 2009…,2020.\nThe model is then adjusted to: model the cyclic pattern from year to year via random effects of Fourier terms, and reduce the influence of historical excess mortality (as mentioned above) by downweighing the residuals.\nResults/limitations, if any.\nSeveral statistics were used to evaluate the model’s forecasting accuracy, including the likelihood ratio test (LRT) and the root mean square error % (RMSE%). The models were fitted to historical mortality year from 2009- week 10 of 2020. The remaining 42 weeks of 2020 were forecasted using the LMM, along with the 5-year average model, and the ground truth data. The models all performed well, so an overall recommendation to include the down-weight procedure for past excess mortality and to include a serial correlation structure were made. The LMM did fit the mortality data better and two years were better predicted compared to the 5-year weekly average models. Many limitations exist, including differences in the reporting of COVID-19 deaths in Belgium and the Netherlands, and across the world. Additionally, it is unclear if the added complexity of LMMs provide a significant benefit over 5-year weekly average models in years besides 2014 and 2016."
  },
  {
    "objectID": "literature.html#when-a-joint-model-should-be-preferred-over-a-linear-mixed-model-for-analysis-of-longitudinal-health-related-quality-of-life-data-in-cancer-clinical-trials---pubmed-nih.gov",
    "href": "literature.html#when-a-joint-model-should-be-preferred-over-a-linear-mixed-model-for-analysis-of-longitudinal-health-related-quality-of-life-data-in-cancer-clinical-trials---pubmed-nih.gov",
    "title": "Literature",
    "section": "",
    "text": "What is the goal of the paper?\nThe goal of this paper is to address a potential shortcoming of LMMs, which is when data is not missing at random, but instead, data is missing that is dependent on the health-related quality-of-life of the patient (i.e., data is missing because quality-of-life – the outcome - decreased). Viewing missing data like this, a survival model may be more appropriate. Or, as this paper suggests, a joint model (JM) that includes both LMM and survival sub-models.\nWhy is it important?\nThis concept is important because clinical trials are frequently employing LMMs to evaluate longitudinal data. Clinical trials involve human subjects, which are known to be more variable compared to benchtop studies. This includes variation in the completeness of patient reported outcome measures (PROMs), especially for longitudinal studies. Many times, this variation is random, and independent of the outcome measure. However, if the outcome measure itself is having an effect on the completeness, then it could represent a major bias limitation of LMMs that should be addressed using the proposed joint models.\nHow is it solved? – methods\nThis paper first introduces the LMM that is traditional in longitudinal clinical studies like this. Then, to account for the fact that observations in quality-of-life scores ends with a dropout event, a joint model is created by linking the LMM to a survival model with shared parameters.\nThis JM was then evaluated using historical clinical trial data where a standard LMM was utilized, and in several simulation studies where extreme examples of this dependence was introduced.\nResults/limitations, if any.\nThis paper showed that poor quality-of-life scores are associated with drop out. Therefore, LMMs should be avoided when analyzing this data in clinical trials. The LMMs in both the historical studies and in the simulations were overly optimistic in estimating the quality-of-life scores. Specifically, the LMM overestimates the slope governing the prediction trajectory in both treatment and control arms. The LMM is also more optimistic for the control arm than the experimental arm, despite the protective effect of treatment on the quality-of-life score."
  },
  {
    "objectID": "literature.html#introduction-to-linear-mixed-models-oarc.ucla.edua",
    "href": "literature.html#introduction-to-linear-mixed-models-oarc.ucla.edua",
    "title": "Literature",
    "section": "INTRODUCTION TO LINEAR MIXED MODELS (oarc.ucla.edua)",
    "text": "INTRODUCTION TO LINEAR MIXED MODELS (oarc.ucla.edua)\nWhat is the goal of the paper?\nThe goal of the paper is to show how linear mixed models can help analyze data sets with non independence. Non independence can happen when data is hierarchical in structure resulting in correlations within groups in the data sets resulting in the violation of the assumption of independence between observations. The example provided has a sample dataset with different patient observations belonging to multiple doctors.\nWhy is it important?\nGoing on the aforementioned example, data can be averaged on a doctor group level but his would result in less data. And you can not make inferences about an individual patient. You can also do multiple linear models for each doctor but the models themselve have less data resulting in more noise.\nHow is it solved? – methods\nLinear Mixed models find a trade off between both approaches where the random effects are still considered for each doctor but there is still an overall grand or fixed effect.\nResults/limitations, if any.\nThere was no dataset or experiment analyzed in this paper. It is just a technical overview."
  },
  {
    "objectID": "literature.html#linear-mixed-model-for-analyzing-longitudinal-data-a-simulation-study-of-children-growth-differences-sciencedirect.com",
    "href": "literature.html#linear-mixed-model-for-analyzing-longitudinal-data-a-simulation-study-of-children-growth-differences-sciencedirect.com",
    "title": "Literature",
    "section": "Linear Mixed Model for Analyzing Longitudinal Data: A Simulation Study of Children Growth Differences (sciencedirect.com)",
    "text": "Linear Mixed Model for Analyzing Longitudinal Data: A Simulation Study of Children Growth Differences (sciencedirect.com)\nWhat is the goal of the paper?\nThe goal of the paper is to use Linear Mixed Models to analyze multilevel data of developmental growth rates in children. Different covariance structures were modeled within the LMM to capture correlated data through time.\nWhy is it important?\nGrowth curves in children are usually represented as a 2 level data structure. At level 2 is the individual child and at the second level is each individual observation. Traditional linear models are not effective due to the non-independence of the data.\nHow is it solved? – methods\nThe simulation study found that the UN covariance performed the best although it suffered from efficiency because of the high number of parameters leaving the ARH(1) as a solid alternative. The data was also not naturally collected but simulated\nResults/limitations, if any.\nThe simulation study found that the UN covariance performed the best although it suffered from efficiency because of the high number of parameters leaving the ARH(1) as a solid alternative. The data was also not naturally collected but simulated"
  },
  {
    "objectID": "literature.html#a-brief-introduction-to-mixed-effects-modelling-and-multi-model-inference-in-ecology-nih.gov",
    "href": "literature.html#a-brief-introduction-to-mixed-effects-modelling-and-multi-model-inference-in-ecology-nih.gov",
    "title": "Literature",
    "section": "A brief introduction to mixed effects modelling and multi-model inference in ecology (nih.gov)",
    "text": "A brief introduction to mixed effects modelling and multi-model inference in ecology (nih.gov)\nWhat is the goal of the paper?\nThe goal of this paper is to serve as a guide on the end to end to end analysis from formulating a hypothesis to inference to explaining the model parameters as it regards to biological and ecological data. Advantages and disadvantages are discussed with the different options available at different parts of the linear mixed model analysis pipeline\nWhy is it important?\nThis article is relevant to our group especially, to those unfamiliar with Linear Mix Models like myself. As it regards to biological science, this paper helps the reader avoid common pitfalls when implementing LMMs\nHow is it solved? – methods\nThe paper has a play by play on the different stages of LMM analysis with tangible example that include data, code and several graphs.\nResults/limitations, if any\nThere is no study or actual experiment being analyzed by linear mixed models. Although it does have references to plenty of academic research papers"
  },
  {
    "objectID": "literature.html#level-logical-explanations-visualizations-of-estimates-in-linear-mixed-models-recommendations-for-reporting-multilevel-data-and-analyses",
    "href": "literature.html#level-logical-explanations-visualizations-of-estimates-in-linear-mixed-models-recommendations-for-reporting-multilevel-data-and-analyses",
    "title": "Literature",
    "section": "LEVEL (Logical Explanations & Visualizations of Estimates in Linear mixed models): recommendations for reporting multilevel data and analyses",
    "text": "LEVEL (Logical Explanations & Visualizations of Estimates in Linear mixed models): recommendations for reporting multilevel data and analyses\nWhat is the goal of the paper?\nResearchers use LLMs to study hierarchical data and often report them under different names like mixed effects models, multilevel data, contextual analysis and hierarchical studies. There is no standardization across these papers for analyzing hierarchical data which leads to different aspects being reported. The goal of the paper is to make a standardized process for analyzing multilevel data\nWhy is it important?\nThis is important so studies across different time periods can be compared more easily.\nHow is it solved? – methods\nThe paper suggests using the LEVEL (Logical Explanations & Visualizations of Estimates in Linear mixed models) as framework for conducting studies with reporting recommendations\nResults/limitations, if any.\nLack of flexibility. Sticking to a framework can inhibit creative analysis since you’re always looking at the framework for guidance."
  },
  {
    "objectID": "literature.html#estimation-and-selection-in-linear-mixed-models-with-missing-data-under-compound-symmetric-structure-nih.gov",
    "href": "literature.html#estimation-and-selection-in-linear-mixed-models-with-missing-data-under-compound-symmetric-structure-nih.gov",
    "title": "Literature",
    "section": "Estimation and selection in linear mixed models with missing data under compound symmetric structure (nih.gov)",
    "text": "Estimation and selection in linear mixed models with missing data under compound symmetric structure (nih.gov)\nWhat is the goal of the paper?\nMissing values occur all the time in real data. Statisticians and scientists use linear mixed models as a way to circumvent this issue. This paper aims to examine the estimation and model selection performance when faced with different rates of missing data. The paper employs two types of missing data. Missing at random and not at random.\nWhy is it important?\nGiven the frequency of missing data it’s important to know the impact it has on the model’s results. It’s also important to be able to\nHow is it solved? – methods\nMissingness of data is recorded using an indicator based matrix and then a likelihood based estimator is made to capture the probability of distribution of the observed data given the model parameters.\nResults/limitations, if any.\nThere is adequate model performance when there is a moderate amount of missingness in the data. However, the paper focuses on compound symmetric structures which assumes equal variance among any given pair of observations."
  },
  {
    "objectID": "literature.html#when-to-use-mixed-models",
    "href": "literature.html#when-to-use-mixed-models",
    "title": "Literature",
    "section": "When to use mixed models",
    "text": "When to use mixed models\nWhat is the goal of the paper?\nThe goal of the paper is to provide insights into the appropriate usage of mixed models in data science projects. It aims to discuss the types of outcome variables that mixed models can handle, highlight their advantages and disadvantages, and offer guidance on when to use them effectively.\nWhy is it important?\nUnderstanding when to employ mixed models is crucial for data scientists as these models offer unique capabilities, such as handling nested data structures and accommodating multiple readings on the same subject. Utilizing mixed models appropriately can improve the accuracy and interpretability of statistical analyses, especially in scenarios involving complex data structures.\nHow is it solved? – methods\nThe paper discusses the advantages and disadvantages of mixed models, outlining scenarios where they are beneficial and situations where they may not be necessary. It provides examples to illustrate the application of mixed models in scenarios involving hierarchical data structures and multiple measurements on the same subject. Additionally, the paper offers guidance on model selection and references related articles for further exploration.\nResults/limitations, if any.\nThe paper presents several advantages of mixed models, including their ability to handle nested data, provide interpretable coefficients, and accommodate missing measurements."
  },
  {
    "objectID": "literature.html#an-introduction-to-linear-mixed-effects-modeling-in-r",
    "href": "literature.html#an-introduction-to-linear-mixed-effects-modeling-in-r",
    "title": "Literature",
    "section": "An Introduction to Linear Mixed-Effects Modeling in R",
    "text": "An Introduction to Linear Mixed-Effects Modeling in R\nWhat is the goal of the paper?\nThe goal of the tutorial is to provide both theoretical understanding and practical guidance on implementing mixed-effects models in R, particularly for researchers with basic statistical knowledge but limited experience in using these models. It aims to address the limitations of traditional statistical methods like repeated measures ANOVAs in analyzing correlated data and to introduce mixed-effects modeling as a more flexible and appropriate approach.\nWhy is it important?\nUnderstanding mixed-effects modeling is crucial for researchers, especially in fields like experimental psychology where traditional methods may not adequately address the complexities of correlated data. By offering accessible explanations and practical examples, the tutorial aims to empower researchers to effectively analyze their data using mixed-effects models, thereby improving the quality and validity of their research findings.\nHow is it solved? – methods\nThe tutorial provides a theoretical introduction to mixed-effects modeling, explaining concepts such as fixed and random effects in simple terms. It contrasts mixed-effects modeling with traditional methods like repeated measures ANOVAs, highlighting the advantages of the former in handling correlated data and various types of response variables. Practical guidance is offered through R code snippets and example data, allowing readers to follow along and implement mixed-effects models in their own research.\nResults/limitations, if any.\nThe tutorial does not present empirical results but rather serves as an educational resource. It effectively communicates the benefits of mixed-effects modeling and provides step-by-step instructions for implementation in R."
  },
  {
    "objectID": "literature.html#a-brief-introduction-to-mixed-effects-modelling-and-multi-model-inference-in-ecology",
    "href": "literature.html#a-brief-introduction-to-mixed-effects-modelling-and-multi-model-inference-in-ecology",
    "title": "Literature",
    "section": "A brief introduction to mixed effects modelling and multi-model inference in ecology",
    "text": "A brief introduction to mixed effects modelling and multi-model inference in ecology\nWhat is the goal of the paper?\nThe paper aims to provide best practices for applying linear mixed effects models (LMMs) to biological data, particularly in ecology and evolutionary studies. It seeks to address the complexities of ecological data and offer guidance on model selection, interpretation, and common pitfalls encountered during modeling.\nWhy is it important?\nWith the increasing use of LMMs in biological data analysis, particularly in ecology, establishing best practices is critical for enhancing the robustness of conclusions drawn from ecological and evolutionary studies. Effective application of LMMs can improve the accuracy and reliability of research findings.\nHow is it solved? – methods The paper discusses various aspects of applying LMMs to biological data, including model selection, error structure, data transformation, and methods for model selection. It emphasizes the importance of careful consideration and consultation with a statistician, particularly in complex situations.\nResults/limitations, if any\nWhile the paper does not present empirical results, it offers practical solutions and recommendations for researchers working with ecological data. It effectively communicates the advantages and disadvantages of LMMs and provides valuable insights for their application in ecology and evolutionary studies."
  },
  {
    "objectID": "literature.html#introduction-to-linear-mixed-models",
    "href": "literature.html#introduction-to-linear-mixed-models",
    "title": "Literature",
    "section": "INTRODUCTION TO LINEAR MIXED MODELS",
    "text": "INTRODUCTION TO LINEAR MIXED MODELS\nWhat is the goal of the paper?\nThe article aims to provide a step-by-step code implementation guide for Linear Mixed Models (LMMs) and to explain the introduction of random effects in these models.\nWhy is it important?\nUnderstanding random effects in mixed models is crucial for researchers as it allows for the consideration of variability within and between groups in hierarchical data structures. Properly defining and incorporating random effects ensures accurate modeling and interpretation, helping to avoid issues such as pseudoreplication and ensuring the independence of observations.\nHow is it solved? – methods\nThe article explains the concepts of crossed and nested random effects in mixed models. Crossed random effects occur when factors are not hierarchically structured and can be observed across multiple levels, while nested random effects occur when one factor is nested within another, forming a hierarchical structure. The importance of properly coding the data to explicitly define nested structures is emphasized.\nResults/limitations, if any.\nThe article does not present empirical results but serves as an educational resource on implementing LMMs with random effects."
  },
  {
    "objectID": "literature.html#robustness-of-linear-mixed-effects-models-to-violations-of-distributional-assumptions",
    "href": "literature.html#robustness-of-linear-mixed-effects-models-to-violations-of-distributional-assumptions",
    "title": "Literature",
    "section": "Robustness of linear mixed-effects models to violations of distributional assumptions",
    "text": "Robustness of linear mixed-effects models to violations of distributional assumptions\nWhat is the goal of the paper?\nThe paper aims to investigate the robustness of linear mixed-effects models (LMMs) in analyzing complex datasets commonly found in ecology and evolution. It evaluates the impact of various violations of distributional assumptions and missing random effect components on model estimates.\nWhy is it important?\nUnderstanding the robustness of LMMs is crucial for researchers working with complex ecological and evolutionary datasets. Despite potential violations of assumptions and missing components, LMMs are widely used in these fields. Assessing their robustness helps ensure the accuracy and reliability of model estimates, even in challenging scenarios.\nHow is it solved? – methods\nThe study evaluates the impact of skewed, bimodal, and heteroscedastic random effect and residual variances, as well as the effects of missing random effect terms and correlated fixed effect predictors on model estimates. It likely employs simulations or analytical approaches to systematically assess the performance of LMMs under various conditions.\nResults/limitations, if any.\nThe results indicate that while violations of assumptions may lead to slight biases and decreased precision in estimates, the overall robustness of LMMs allows for accurate and unbiased estimation of fixed and random effects."
  },
  {
    "objectID": "literature.html#model-selection-in-linear-mixed-effect-models",
    "href": "literature.html#model-selection-in-linear-mixed-effect-models",
    "title": "Literature",
    "section": "Model selection in linear mixed effect models",
    "text": "Model selection in linear mixed effect models\nWhat is the goal of the paper?\nThe goal of the paper is to improve variable selection and parameter estimation in linear mixed effect models, which are critical for analyzing longitudinal, panel, and cross-sectional data in various scientific domains.\nWhy is it important?\nImproving variable selection and parameter estimation is crucial because it directly impacts the accuracy and reliability of data analysis across scientific fields. Efficiently identifying relevant variables and accurately estimating their effects are essential for drawing valid conclusions from complex data structures.\nHow is it solved? – Methods\nThe authors introduce a simple, iterative procedure that employs the smoothly clipped absolute deviation (SCAD) penalty function to estimate and select both fixed and random effects in these models. This approach is highlighted for being a consistent variable selection method with some oracle properties, suggesting it can perform almost as well as if the true underlying model were known.\nResults/limitations, if any.\nThe approach’s effectiveness and efficiency are validated through simulation studies and real data analysis. Nevertheless, the paper also points out limitations, including the method’s dependence on certain conditions for its asymptotic properties to hold and the potential computational challenges encountered with high-dimensional datasets."
  },
  {
    "objectID": "literature.html#random-effects-structure-for-testing-interactions-in-linear-mixed-effects-models",
    "href": "literature.html#random-effects-structure-for-testing-interactions-in-linear-mixed-effects-models",
    "title": "Literature",
    "section": "Random Effects Structure for Testing Interactions in Linear Mixed-Effects Models",
    "text": "Random Effects Structure for Testing Interactions in Linear Mixed-Effects Models\nWhat is the goal of the paper?\nThe goal is to provide a more accurate method for testing interactions within linear mixed-effects models, critiquing existing guidelines and proposing new ones that emphasize the inclusion of random slopes for the highest-order combination of within-unit factors in interactions.\nWhy is it important?\nThis is important because accurately testing interactions in mixed-effects models is crucial for statistical analyses, especially in avoiding high Type I error rates. The paper aims to refine the approach to these models to ensure more reliable results.\nHow is it solved? – Methods\nThe author employs Monte Carlo simulations to test the proposed guidelines, demonstrating that neglecting critical random slopes can significantly increase the chance of a false rejection of the null hypothesis. Including appropriate random slopes in the model is shown to ensure better performance.\nResults/limitations, if any.\nThe findings highlight that including the correct random slopes in mixed-effects models greatly improves model performance, particularly in accurately testing interactions between categorical variables. However, the paper’s limitations include its focus on interactions between categorical variables and the specific conditions of the simulations used."
  },
  {
    "objectID": "literature.html#pymer4-connecting-r-and-python-for-linear-mixed-modeling",
    "href": "literature.html#pymer4-connecting-r-and-python-for-linear-mixed-modeling",
    "title": "Literature",
    "section": "Pymer4: Connecting R and Python for Linear Mixed Modeling",
    "text": "Pymer4: Connecting R and Python for Linear Mixed Modeling\nWhat is the goal of the paper?\nThe goal is to develop Pymer4, a tool that bridges R and Python for linear mixed modeling, addressing the gap in Python for a package as flexible as R’s lme4 for complex data analysis.\nWhy is it important?\nPymer4 is significant for providing Python users with an accessible, integrated tool for linear mixed modeling, which was previously lacking, enhancing the analytical capabilities within the Python ecosystem.\nHow is it solved? – Methods\nPymer4 offers a solution by by leveraging the rpy2 library, as it connects to R’s lme4 package, offering a Pythonic interface for mixed modeling that integrates well with scientific Python tools, simplifying the analysis process.\nResults/limitations, if any.\nPymer4 successfully extends lme4’s functionality to Python users, offering features like significance testing and data visualization integration, enhancing multilevel model analysis. The paper focuses on the tool’s capabilities without detailing specific limitations."
  },
  {
    "objectID": "literature.html#a-powerful-and-flexible-linear-mixed-model-framework-for-the-analysis-of-relative-quantification-rt-pcr-data",
    "href": "literature.html#a-powerful-and-flexible-linear-mixed-model-framework-for-the-analysis-of-relative-quantification-rt-pcr-data",
    "title": "Literature",
    "section": "A powerful and flexible linear mixed model framework for the analysis of relative quantification RT-PCR data",
    "text": "A powerful and flexible linear mixed model framework for the analysis of relative quantification RT-PCR data\nWhat is the goal of the paper?\nThe paper introduces a novel linear mixed model framework for analyzing relative quantification RT-PCR data, aiming to overcome the limitations of existing statistical methods by providing more accurate and flexible analysis tools.\nWhy is it important?\nThis framework is crucial for its potential to enhance the statistical power and flexibility in analyzing RT-PCR data, enabling researchers to conduct more reliable and varied analyses of gene expression across different experimental conditions.\nHow is it solved? – Methods\nThe method involves a sophisticated statistical approach that incorporates both fixed and random effects in a linear mixed model, allowing for a more nuanced analysis of RT-PCR data that accounts for various sources of variability.\nResults/limitations, if any.\nThe framework has been shown to yield more accurate and statistically powerful results compared to traditional methods, facilitating better decision-making in biological research. The paper thoroughly evaluates the model’s performance and discusses its applicability to a wide range of experimental designs."
  },
  {
    "objectID": "report.html#implementation-methods",
    "href": "report.html#implementation-methods",
    "title": "Report",
    "section": "",
    "text": "The implementation of LMMs has been facilitated by various software packages and programming languages. Brown (Brown 2021) provides a comprehensive guide to implementing LMMs in R, a widely used statistical programming language, offering a step-by-step walkthrough of model syntax without delving deeply into complex mathematical foundations. Additionally, Pymer4, developed by Jolly (Jolly 2018), bridges R and Python, offering Python users a flexible and integrated tool for linear mixed modeling by leveraging the capabilities of R’s lme4 package. This tool enhances the analytical capabilities within the Python ecosystem, making advanced statistical methods more accessible to a broader audience."
  },
  {
    "objectID": "report.html#industry-specific-uses",
    "href": "report.html#industry-specific-uses",
    "title": "Report",
    "section": "",
    "text": "LMMs find applications across various scientific domains, each with its unique data structures and analytical challenges. Magezi (Magezi 2015) highlights the use of LMMs in within-participant psychology experiments, addressing the complexities of repeated measures and nested data structures common in psychological research. Harrison Et. A. (Harrison et al. 2018) and Bolker Et. Al. (Bolker et al. 2009) discuss the application of LMMs and generalized linear mixed models (GLMMs) in ecology, emphasizing their utility in analyzing ecological data that involve complex relationships and hierarchical data structures. In the medical field, LMMs are employed to model pandemic-induced mortality changes, as demonstrated by Verbeeck Et. Al. (Verbeeck et al. 2023), and to analyze longitudinal health-related quality of life data in cancer clinical trials, as discussed by Touraine Et. Al. [Touraine et al. (2023)]."
  },
  {
    "objectID": "report.html#strengths-and-weaknesses",
    "href": "report.html#strengths-and-weaknesses",
    "title": "Report",
    "section": "",
    "text": "The strengths of LMMs lie in their flexibility to model complex data structures and their ability to handle missing data, making them a powerful tool for a wide range of scientific inquiries. However, their application is not without challenges. Peng and Lu (Peng and Lu 2012) address the difficulty of variable selection and parameter estimation in LMMs, proposing an iterative procedure to improve model accuracy. Barr (Barr 2013) critiques existing guidelines for testing interactions within LMMs, proposing new guidelines to ensure more reliable results. Despite their robustness, as noted by Schielzeth Et. Al. [Schielzeth et al. (2020)], LMMs require careful evaluation of model assumptions and may present computational challenges, especially with high-dimensional datasets."
  },
  {
    "objectID": "report.html#purpose",
    "href": "report.html#purpose",
    "title": "Report",
    "section": "",
    "text": "This literature review collectively emphasize the versatility, robustness, and broad applicability of LMMs and GLMMs across various fields of research. Despite their advantages, the importance of careful model selection, acknowledgment of limitations, and the potential need for more complex models such as joint models in certain scenarios are also highlighted. As the use of LMMs continues to grow, the development of standardized processes and user-friendly tools will be crucial in ensuring the accurate and effective application of these models in research."
  },
  {
    "objectID": "literature.html#to-transform-or-not-to-transform-using-generalized-linear-mixed-models-to-analyse-reaction-time-data",
    "href": "literature.html#to-transform-or-not-to-transform-using-generalized-linear-mixed-models-to-analyse-reaction-time-data",
    "title": "Literature",
    "section": "To transform or not to transform: using generalized linear mixed models to analyse reaction time data",
    "text": "To transform or not to transform: using generalized linear mixed models to analyse reaction time data\nWhat is the goal of the paper?\nThe paper aims to challenge the common practice of transforming reaction time (RT) data to meet normality assumptions in statistical analyses within cognitive psychology research. It proposes generalized linear mixed-effect models (GLMMs) as a solution to accurately analyze RT data without the need for transformation, thus avoiding potential theoretical implications and misleading conclusions.\nWhy is it important?\nIt highlights the discrepancy between analyses of raw RT data and transformed RT data, as demonstrated by Balota et al. (2013), emphasizing the need for a more nuanced approach to analyzing RT data. By advocating for GLMMs, the paper aims to promote proper assessment of individual differences and enhance the testing of cognitive theories.\nHow is it solved? – Methods\nThe study discusses the theoretical decisions involved in specifying a GLMM and provides reanalysis of datasets from Balota et al. (2013) to illustrate the application of GLMMs in RT data analysis. It emphasizes the importance of analyzing changes in RT distribution at a finer level to capture more accurate measures of group performance and effectively test cognitive theories.\nResults/limitations, if any.\nThe paper suggests that GLMMs offer a more robust approach to analyzing RT data compared to traditional methods like linear mixed-effect models (LMMs) with transformed data. However, it acknowledges the complexities of addressing skewed dependent variables like RT in LMMs and the potential challenges in adopting GLMMs, such as the need for careful model specification."
  },
  {
    "objectID": "literature.html#analysing-disease-incidence-data-from-designed-experiments-by-generalized-linear-mixed-models",
    "href": "literature.html#analysing-disease-incidence-data-from-designed-experiments-by-generalized-linear-mixed-models",
    "title": "Literature",
    "section": "Analysing disease incidence data from designed experiments by generalized linear mixed models",
    "text": "Analysing disease incidence data from designed experiments by generalized linear mixed models\nWhat is the goal of the paper?\nThe paper aims to introduce generalized linear mixed models (GLMMs) as a robust method for analyzing disease incidence data from designed experiments, specifically addressing overdispersion issues common in epidemiological research.\nWhy is it important?\nIt highlights the inadequacy of traditional methods like ANOVA for such data, underlining the need for alternative approaches like GLMMs to better capture the complexities of disease clustering and aggregation.\nHow is it solved? – Methods\nThe study presents GLMMs as a versatile tool, capable of accommodating both fixed and random effects, thus offering a more flexible framework for analyzing disease incidence data. It illustrates the application of GLMMs using real-world data from an experiment on downy mildew incidence in grapevines.\nResults/limitations, if any.\nThe analysis using GLMMs reveals significant treatment effects and provides parameter estimates. However, the approach is not without limitations, including assumptions made in modeling and the potential challenge of interpreting results accurately, particularly in complex experimental designs."
  },
  {
    "objectID": "literature.html#fitting-linear-mixed-effects-models-using-lme4-journal-of-statistical-software-jstatsoft.org",
    "href": "literature.html#fitting-linear-mixed-effects-models-using-lme4-journal-of-statistical-software-jstatsoft.org",
    "title": "Literature",
    "section": "",
    "text": "What is the goal of the paper?\nThe paper presents the lme4 package for R, which facilitates the fitting of linear mixed-effects models. The authors aim to articulate the package’s capabilities in evaluating the profiled deviance or REML criterion for linear mixed models and to explain the representation and optimization of such models for parameter estimation.\nWhy is it important?\nThe significance of this paper lies in its contribution to the field of computational methods for fitting mixed models—an area with many open problems. The lme4 package represents an evolution in this domain, offering more efficient computational tools and a syntax that simplifies the modeling process, especially for models with crossed random effects.\nHow is it solved? – methods\nThe package utilizes maximum likelihood or restricted maximum likelihood (REML) estimates for linear mixed-effects model parameters, employing numerical representation and optimization functions within R. The paper delves into the model’s structure, the evaluative steps for the profiled deviance or REML criterion, and the class structure representing such models, highlighting the improvements over previous formulations.\nResults/limitations, if any.\nThe document focuses more on methodology than specific results or limitations. It details the improvement over the nlme package, addressing efficient linear algebra tools and the incorporation of profile likelihood confidence intervals on random-effects parameters. The paper emphasizes the ongoing development of the lme4 package, acknowledging the need for stability and usability for a broad range of applications."
  },
  {
    "objectID": "literature.html#statistical-primer-an-introduction-to-the-application-of-linear-mixed-effects-models-in-cardiothoracic-surgery-outcomes-research-a-case-study-using-homograft-pulmonary-valve-replacement-data---pubmed-nih.gov",
    "href": "literature.html#statistical-primer-an-introduction-to-the-application-of-linear-mixed-effects-models-in-cardiothoracic-surgery-outcomes-research-a-case-study-using-homograft-pulmonary-valve-replacement-data---pubmed-nih.gov",
    "title": "Literature",
    "section": "",
    "text": "What is the goal of the paper?\nThe goal of the paper is to provide a detailed introduction to developing and interpreting linear mixed-effects models for repeated measurements in the context of cardiothoracic surgery outcomes research. The paper uses a dataset on patients undergoing surgical pulmonary valve replacement to illustrate the steps of developing such models for clinician researchers.\nWhy is it important?\nThis work is important because the emergence of large cardio-thoracic surgery datasets, including repeated measurements over time, presents an opportunity to apply advanced modeling of outcomes. Linear mixed-effects models offer a more nuanced understanding of these outcomes compared to traditional methods, which is crucial for enhancing clinical decision-making and patient care.\nHow is it solved? – methods\nThe authors used a retrospective dataset containing serial echocardiographic measurements from patients who underwent surgical pulmonary valve replacement at Erasmus MC between 1986 and 2017. The paper discusses the construction of the model, including dealing with missing values, correlated variables, and multicollinearity. It also covers model specification, variable selection, addressing nonlinearity, and interpretation of results. An R script is provided for implementing the model.\nResults/limitations, if any.\nThe paper illustrates the construction of the model, including essential aspects such as theories of linear mixed-effects models, missing values, collinearity, interaction, nonlinearity, model specification, and results interpretation. It shows that linear mixed-effects models provide a more detailed view of repeated measurements and give more valid estimates compared to linear regression models, especially in the context of cardio-thoracic surgery outcomes research. Limitations related to model assumptions, such as linearity and normal distribution of residuals, are addressed through transformations and statistical tests."
  },
  {
    "objectID": "report.html#mathematical-foundations",
    "href": "report.html#mathematical-foundations",
    "title": "Report",
    "section": "2.1 Mathematical Foundations",
    "text": "2.1 Mathematical Foundations\nLMMs can be defined as:\n\\(y=X\\beta + Z\\gamma + \\epsilon\\)\nwhere:\n\nY is the response vector.\nX is the design matrix for fixed effects.\nβ is the vector of fixed effects (parameters associated with the entire population or certain repeatable levels of experimental factors).\nZ is the design matrix for random effects.\nγ is the vector of random effects (represent random deviations from the population parameters (β ) for different subjects or experimental units; i.e., the variability not explained by the fixed effects).\nϵ is the vector of residual errors.\n\nAlthough more flexible than other methods such as ANOVA, there are several assumptions for LMMs:\n\nRandom effects (γ) are assumed to follow a normal distribution with mean zero and variance-covariance matrix G.\n\\(\\gamma \\sim N(0,G)\\)\nResidual errors (ϵ ) are assumed to follow a normal distribution with mean zero and variance-covariance matrix R.\n\\(\\epsilon \\sim N(0,R)\\)\nRandom effects (γ) and residual errors (ϵ ) are assumed to be independent."
  },
  {
    "objectID": "report.html#breakdown-of-mathematical-foundations",
    "href": "report.html#breakdown-of-mathematical-foundations",
    "title": "Report",
    "section": "2.2 Breakdown of Mathematical Foundations",
    "text": "2.2 Breakdown of Mathematical Foundations\n[PLACEHOLDER FOR LAYMANS TERMS EXPLANATIONS]"
  },
  {
    "objectID": "report.html#sample-data-structure",
    "href": "report.html#sample-data-structure",
    "title": "Report",
    "section": "2.3 Sample Data Structure",
    "text": "2.3 Sample Data Structure\nLMMs require a tidy data set where each variables are columns and observations are rows. Smaller datasets are usually saved as CSV files and are often loaded from a database. The dataset can contain nulls but they still need to be handled whether they are omitted or imputed and depends on the amount and which columns are affected. The lmer() function in the lme4 library will automatically drop any null values so it is important that data is inspected and visualized before contstructing any models. Below is an example of tidy data."
  },
  {
    "objectID": "report.html#packages",
    "href": "report.html#packages",
    "title": "Report",
    "section": "3.1 Packages",
    "text": "3.1 Packages\n\n\nCode\nif (!requireNamespace(c(\"tidyverse\", \"lme4\", \"nlme\", \"gt\", \"RefManageR\", \"DataExplorer\", \"gtsummary\"), quietly = TRUE)) {\n    install.packages(c(\"tidyverse\", \"lme4\", \"nlme\", \"gt\", \"RefManageR\", \"DataExplorer\", \"gtsummary\"))\n}\n\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(nlme)\nlibrary(gt)\nlibrary(RefManageR)\nlibrary(DataExplorer)\nlibrary(gtsummary)\n\n#references &lt;- ReadBib(\"references.bib\")\n#summary(references)\n\n\n\ntidyverse: used for data wrangling and visualisation.\nlme4: used for LMM within R.\nnlme: used for nonlinear LMM (NLMM) within R.\ngt: used for table generation.\ngtsummary: used for summary table generation of descriptive statistics.\nRefManageR: used for BibTex reference management."
  },
  {
    "objectID": "report.html#data-ingestion",
    "href": "report.html#data-ingestion",
    "title": "Report",
    "section": "3.2 Data Ingestion",
    "text": "3.2 Data Ingestion\n\n\nCode\n# Load the dataset\nBMI &lt;- read.csv(\"data/BMI_IOS_SCD_Asthma.csv\")\n\n\n\nBMI from Kaggle (Impact of BMI on IOS measures on children (kaggle.com))\n\nDescription: This dataset is from a retrospective study to assess the impact of BMI on impulse oscillometry (IOS) estimates of airway resistance and reactance in children with sickle cell disease (C-SCD)\nDetailed Description: The dataset comprises various attributes and measurements across its columns. Categorical variables, such as Group, Subject ID, Observation_number, Hydroxyurea, Asthma, ICS, LABA, and Gender, denote different groupings, individual subjects, and attributes like medication usage and gender. Numerical variables like Age (months), Height (cm), Weight (Kg), BMI, R5Hz_PP, R20Hz_PP, X5Hz_PP, and Fres_PP provide quantitative data on subjects’ characteristics and test results. Notably, the summary also identifies missing values, such as the 14 instances in the Fres_PP variable, which warrant consideration in subsequent analysis.These columns provide measurements and estimates related to airway resistance and reactance obtained using impulse oscillometry (IOS), which is a non-invasive method for assessing respiratory function. These parameters are valuable in understanding the impact of BMI on respiratory measures in children with sickle cell disease (C-SCD) and African-American children with asthma (C-Asthma) participating in the study.\nWhy suitable for LMMs: The dataset has multiple observations, over time, for the same set of participants."
  },
  {
    "objectID": "report.html#exploratory-data-analysis-eda",
    "href": "report.html#exploratory-data-analysis-eda",
    "title": "Report",
    "section": "3.2 Exploratory Data Analysis (EDA)",
    "text": "3.2 Exploratory Data Analysis (EDA)\n\n\nCode\nx &lt;- BMI\n\nstr(x)\n\n\n'data.frame':   219 obs. of  16 variables:\n $ Group             : chr  \"C-SCD\" \"C-SCD\" \"C-SCD\" \"C-SCD\" ...\n $ Subject.ID        : int  1 1 1 1 2 3 3 4 4 5 ...\n $ Observation_number: int  1 2 3 4 1 1 2 1 2 1 ...\n $ Hydroxyurea       : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Asthma            : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ ICS               : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ LABA              : chr  \"No\" \"No\" \"Yes\" \"Yes\" ...\n $ Gender            : chr  \"Male\" \"Male\" \"Male\" \"Male\" ...\n $ Age..months.      : int  239 193 212 224 204 178 186 222 210 196 ...\n $ Height..cm.       : num  164 163 164 164 154 ...\n $ Weight..Kg.       : num  61.5 62.3 63.1 63.7 66.4 51.9 56.7 66.7 66.9 52.9 ...\n $ BMI               : num  22.8 23.5 23.6 23.7 27.8 ...\n $ R5Hz_PP           : int  145 103 107 87 124 109 117 101 179 136 ...\n $ R20Hz_PP          : int  133 98 98 87 121 86 105 132 153 97 ...\n $ X5Hz_PP           : num  -456 111 174 -303 98 115 107 -216 195 140 ...\n $ Fres_PP           : int  NA 169 159 NA 135 148 159 NA 175 199 ...\n\n\nCode\nhead(x)\n\n\n  Group Subject.ID Observation_number Hydroxyurea Asthma ICS LABA Gender\n1 C-SCD          1                  1         Yes    Yes Yes   No   Male\n2 C-SCD          1                  2         Yes    Yes Yes   No   Male\n3 C-SCD          1                  3         Yes    Yes Yes  Yes   Male\n4 C-SCD          1                  4         Yes    Yes Yes  Yes   Male\n5 C-SCD          2                  1          No     No  No   No Female\n6 C-SCD          3                  1         Yes    Yes  No   No   Male\n  Age..months. Height..cm. Weight..Kg.   BMI R5Hz_PP R20Hz_PP X5Hz_PP Fres_PP\n1          239       164.1        61.5 22.84     145      133    -456      NA\n2          193       162.7        62.3 23.53     103       98     111     169\n3          212       163.5        63.1 23.60     107       98     174     159\n4          224       163.8        63.7 23.74      87       87    -303      NA\n5          204       154.5        66.4 27.82     124      121      98     135\n6          178       158.0        51.9 20.79     109       86     115     148\n\n\nCode\nvariables &lt;- colnames(x)\n\nvariables_table &lt;- data.frame(\n  Variable = variables,\n  Description = c(\n    \"This column indicates the group to which the subject belongs. There are two groups in the study: children with sickle cell disease (C-SCD) and African-American children with asthma (C-Asthma).\",\n    \"Each subject in the study is assigned a unique identifier or ID, which is listed in this column. The ID is used to differentiate between individual participants.\",\n    \"This column represents the number assigned to each observation or measurement taken for a particular subject. Since this is a longitudinal study, multiple observations may be recorded for each subject over time.\",\n    \"This column indicates whether the subject with sickle cell disease (C-SCD) received hydroxyurea treatment. Hydroxyurea is a medication commonly used for the treatment of sickle cell disease.\",\n    \"This column indicates whether the subject has a diagnosis of asthma. It distinguishes between children with sickle cell disease (C-SCD) and African-American children with asthma (C-Asthma).\",\n    \"This column indicates whether the subject is using inhaled corticosteroids (ICS). ICS is a type of medication commonly used for the treatment of asthma and certain other respiratory conditions.\",\n    \"This column indicates whether the subject is using a long-acting beta-agonist (LABA). LABA is a type of medication often used in combination with inhaled corticosteroids for the treatment of asthma.\",\n    \"This column represents the gender of the subject, indicating whether they are male or female\",\n    \"This column specifies the age of the subject at the time of the observation or measurement. Age is typically measured in months.\",\n    \"This column represents the height of the subject, typically measured in a standard unit of length, such as centimeters or inches. Height is an important variable to consider in assessing the impact of BMI on respiratory measures.\",\n    \"This column indicates the weight of the subject at the time of the observation or measurement. Weight is typically measured in kilograms (Kg) and is an important variable for calculating the body mass index (BMI).\",\n    \"Body Mass Index (BMI) is a measure that assesses body weight relative to height. It is calculated by dividing the weight of an individual (in kilograms) by the square of their height (in meters). The BMI column provides the calculated BMI value for each subject based on their weight and height measurements. BMI is commonly used as an indicator of overall body fatness and is often used to classify individuals into different weight categories (e.g., underweight, normal weight, overweight, obese).\",\n    \"This column represents the estimate of airway resistance at 5 Hz using impulse oscillometry (IOS). Airway resistance is a measure of the impedance encountered by airflow during respiration. The R5Hz_PP value indicates the airway resistance at the frequency of 5 Hz and is obtained through the IOS testing.\",\n    \"This column represents the estimate of airway resistance at 20 Hz using impulse oscillometry (IOS). Similar to R5Hz_PP, R20Hz_PP provides the measure of airway resistance at the frequency of 20 Hz based on the IOS testing.\",\n    \"This column represents the estimate of airway reactance at 5 Hz using impulse oscillometry (IOS). Airway reactance is a measure of the elasticity and stiffness of the airway walls. The X5Hz_PP value indicates the airway reactance at the frequency of 5 Hz and is obtained through the IOS testing.\",\n    \"This column represents the estimate of resonant frequency using impulse oscillometry (IOS). Resonant frequency is a measure of the point at which the reactance of the airways transitions from positive to negative during respiration. The Fres_PP value indicates the resonant frequency and is obtained through the IOS testing.:\"\n    )\n)\n\nvariables_table %&gt;%\n  gt %&gt;%\n  tab_header(\n    title = \"Variable Description\"\n  )\n\n\n\n\n\n\n\n\nVariable Description\n\n\nVariable\nDescription\n\n\n\n\nGroup\nThis column indicates the group to which the subject belongs. There are two groups in the study: children with sickle cell disease (C-SCD) and African-American children with asthma (C-Asthma).\n\n\nSubject.ID\nEach subject in the study is assigned a unique identifier or ID, which is listed in this column. The ID is used to differentiate between individual participants.\n\n\nObservation_number\nThis column represents the number assigned to each observation or measurement taken for a particular subject. Since this is a longitudinal study, multiple observations may be recorded for each subject over time.\n\n\nHydroxyurea\nThis column indicates whether the subject with sickle cell disease (C-SCD) received hydroxyurea treatment. Hydroxyurea is a medication commonly used for the treatment of sickle cell disease.\n\n\nAsthma\nThis column indicates whether the subject has a diagnosis of asthma. It distinguishes between children with sickle cell disease (C-SCD) and African-American children with asthma (C-Asthma).\n\n\nICS\nThis column indicates whether the subject is using inhaled corticosteroids (ICS). ICS is a type of medication commonly used for the treatment of asthma and certain other respiratory conditions.\n\n\nLABA\nThis column indicates whether the subject is using a long-acting beta-agonist (LABA). LABA is a type of medication often used in combination with inhaled corticosteroids for the treatment of asthma.\n\n\nGender\nThis column represents the gender of the subject, indicating whether they are male or female\n\n\nAge..months.\nThis column specifies the age of the subject at the time of the observation or measurement. Age is typically measured in months.\n\n\nHeight..cm.\nThis column represents the height of the subject, typically measured in a standard unit of length, such as centimeters or inches. Height is an important variable to consider in assessing the impact of BMI on respiratory measures.\n\n\nWeight..Kg.\nThis column indicates the weight of the subject at the time of the observation or measurement. Weight is typically measured in kilograms (Kg) and is an important variable for calculating the body mass index (BMI).\n\n\nBMI\nBody Mass Index (BMI) is a measure that assesses body weight relative to height. It is calculated by dividing the weight of an individual (in kilograms) by the square of their height (in meters). The BMI column provides the calculated BMI value for each subject based on their weight and height measurements. BMI is commonly used as an indicator of overall body fatness and is often used to classify individuals into different weight categories (e.g., underweight, normal weight, overweight, obese).\n\n\nR5Hz_PP\nThis column represents the estimate of airway resistance at 5 Hz using impulse oscillometry (IOS). Airway resistance is a measure of the impedance encountered by airflow during respiration. The R5Hz_PP value indicates the airway resistance at the frequency of 5 Hz and is obtained through the IOS testing.\n\n\nR20Hz_PP\nThis column represents the estimate of airway resistance at 20 Hz using impulse oscillometry (IOS). Similar to R5Hz_PP, R20Hz_PP provides the measure of airway resistance at the frequency of 20 Hz based on the IOS testing.\n\n\nX5Hz_PP\nThis column represents the estimate of airway reactance at 5 Hz using impulse oscillometry (IOS). Airway reactance is a measure of the elasticity and stiffness of the airway walls. The X5Hz_PP value indicates the airway reactance at the frequency of 5 Hz and is obtained through the IOS testing.\n\n\nFres_PP\nThis column represents the estimate of resonant frequency using impulse oscillometry (IOS). Resonant frequency is a measure of the point at which the reactance of the airways transitions from positive to negative during respiration. The Fres_PP value indicates the resonant frequency and is obtained through the IOS testing.:\n\n\n\n\n\n\n\nCode\nplot_str(x)\nintroduce(x)\n\n\n  rows columns discrete_columns continuous_columns all_missing_columns\n1  219      16                6                 10                   0\n  total_missing_values complete_rows total_observations memory_usage\n1                   14           205               3504        27592\n\n\nCode\nplot_intro(x)\n\n\n\n\n\n\n\n\n\n\n3.2.1 Missing Values\n\n\nCode\nplot_missing(x)\n\n\n\n\n\n\n\n\n\nBased on the missing values count, it appears that there are no missing values in most of the columns, except for Fres_PP, where there are 14 missing values (6.39%). In this case, omitting missing values for Fres_PP is reasonable, considering the small proportion of missing data compared to the total number of observation\n\n\n3.2.2 Cleaning Data\n\n\nCode\ndim(x)\n\n\n[1] 219  16\n\n\nCode\nx_clean &lt;- na.omit(x) # drops NAs, further analysis is without NA values\n\ndim(x_clean)\n\n\n[1] 205  16\n\n\nCode\nstr(x_clean)\n\n\n'data.frame':   205 obs. of  16 variables:\n $ Group             : chr  \"C-SCD\" \"C-SCD\" \"C-SCD\" \"C-SCD\" ...\n $ Subject.ID        : int  1 1 2 3 3 4 5 6 6 7 ...\n $ Observation_number: int  2 3 1 1 2 2 1 1 2 1 ...\n $ Hydroxyurea       : chr  \"Yes\" \"Yes\" \"No\" \"Yes\" ...\n $ Asthma            : chr  \"Yes\" \"Yes\" \"No\" \"Yes\" ...\n $ ICS               : chr  \"Yes\" \"Yes\" \"No\" \"No\" ...\n $ LABA              : chr  \"No\" \"Yes\" \"No\" \"No\" ...\n $ Gender            : chr  \"Male\" \"Male\" \"Female\" \"Male\" ...\n $ Age..months.      : int  193 212 204 178 186 210 196 175 200 173 ...\n $ Height..cm.       : num  163 164 154 158 164 ...\n $ Weight..Kg.       : num  62.3 63.1 66.4 51.9 56.7 66.9 52.9 39 50.6 56.3 ...\n $ BMI               : num  23.5 23.6 27.8 20.8 20.9 ...\n $ R5Hz_PP           : int  103 107 124 109 117 179 136 96 123 117 ...\n $ R20Hz_PP          : int  98 98 121 86 105 153 97 109 115 117 ...\n $ X5Hz_PP           : num  111 174 98 115 107 195 140 111 164 92 ...\n $ Fres_PP           : int  169 159 135 148 159 175 199 104 109 152 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:14] 1 4 8 13 18 21 44 46 50 52 ...\n  ..- attr(*, \"names\")= chr [1:14] \"1\" \"4\" \"8\" \"13\" ...\n\n\nCode\nplot_bar(x_clean)\n\n\n\n\n\n\n\n\n\nCode\nplot_histogram(x_clean)\n\n\n\n\n\n\n\n\n\nCode\nplot_qq(na.omit(x))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_prcomp(na.omit(x))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nnumeric_vars &lt;- x_clean %&gt;% \n  select_if(is.numeric)\n\n# Boxplot for each numeric variable\npar(mfrow=c(2, 2))\nfor (col in colnames(numeric_vars)) {\n  boxplot(numeric_vars[[col]], main=col)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the boxplot, it’s evident that all columns except “Age (months)” and “Height (cm)” contain outliers. Now, let’s pinpoint these outliers.\n\n\nCode\n# Define a function to detect outliers in each column\ndetect_outliers &lt;- function(column) {\n  Q1 &lt;- quantile(column, 0.25)\n  Q3 &lt;- quantile(column, 0.75)\n  IQR &lt;- Q3 - Q1\n  lower_bound &lt;- Q1 - 1.5 * IQR\n  upper_bound &lt;- Q3 + 1.5 * IQR\n  outliers &lt;- column[column &lt; lower_bound | column &gt; upper_bound]\n  return(outliers)\n}\n\n# Iterate over each column and print outliers; not removed\nfor (col in names(numeric_vars)) {\n  outliers &lt;- detect_outliers(numeric_vars[[col]])\n  if (length(outliers) &gt; 0) {\n    cat(\"Outliers in\", col, \":\\n\")\n    print(outliers)\n    cat(\"\\n\")\n  }\n}\n\n\nOutliers in Weight..Kg. :\n [1]  91.4 100.2 104.4 111.2 120.4 108.6 105.1 100.0  95.5 130.1 108.5 124.6\n[13]  95.2\n\nOutliers in BMI :\n [1] 27.82 37.85 41.49 42.09 26.65 28.66 41.85 41.18 39.89 40.05 38.58 38.74\n[13] 29.92 47.21 41.34 48.07 27.82 26.95\n\nOutliers in R5Hz_PP :\n[1] 187 183 196  10 199 198\n\nOutliers in R20Hz_PP :\n[1] 153 150  12 162 153\n\nOutliers in X5Hz_PP :\n [1]  439 1117  260  270  297  465  338  285  306  -68  543  381\n\n\nCode\nx_clean %&gt;% \n  select(-2) %&gt;%\n  tbl_summary( #gtSummary Table\n    by=Group,\n    type = list(\n      c('Age..months.', 'Height..cm.', 'Weight..Kg.', 'BMI', 'R5Hz_PP', 'R20Hz_PP', 'X5Hz_PP', 'Fres_PP') ~ 'continuous2'),\n    statistic = all_continuous2() ~ c(\n                       \"{mean} ± {sd}\",\n                       \"{median} ({p25}, {p75})\",\n                       \"{min}, {max}\"\n                       ),\n    digits = all_continuous2() ~ 2,\n    missing=\"ifany\",\n  ) %&gt;%\n  bold_labels %&gt;%\n  italicize_levels() %&gt;%\n  as_gt() %&gt;%\n  tab_header(\n    title = \"Summary Statistics\"\n  )\n\n\n\n\n\n\n\n\nSummary Statistics\n\n\nCharacteristic\nC-Asthma, N = 561\nC-SCD, N = 1491\n\n\n\n\nObservation_number\n\n\n\n\n    1\n34 (61%)\n51 (34%)\n\n\n    2\n13 (23%)\n44 (30%)\n\n\n    3\n7 (13%)\n31 (21%)\n\n\n    4\n2 (3.6%)\n17 (11%)\n\n\n    5\n0 (0%)\n5 (3.4%)\n\n\n    6\n0 (0%)\n1 (0.7%)\n\n\nHydroxyurea\n0 (0%)\n112 (75%)\n\n\nAsthma\n56 (100%)\n106 (71%)\n\n\nICS\n40 (71%)\n73 (49%)\n\n\nLABA\n24 (43%)\n14 (9.4%)\n\n\nGender\n\n\n\n\n    Female\n22 (39%)\n59 (40%)\n\n\n    male\n34 (61%)\n0 (0%)\n\n\n    Male\n0 (0%)\n90 (60%)\n\n\nAge..months.\n\n\n\n\n    Mean ± SD\n132.98 ± 35.59\n142.16 ± 44.70\n\n\n    Median (IQR)\n134.00 (100.00, 147.25)\n144.00 (105.00, 179.00)\n\n\n    Range\n87.00, 213.00\n50.00, 215.00\n\n\nHeight..cm.\n\n\n\n\n    Mean ± SD\n146.57 ± 17.33\n145.00 ± 18.83\n\n\n    Median (IQR)\n144.50 (131.00, 162.00)\n146.20 (132.00, 160.30)\n\n\n    Range\n120.00, 185.00\n101.80, 178.90\n\n\nWeight..Kg.\n\n\n\n\n    Mean ± SD\n52.98 ± 30.88\n40.11 ± 16.06\n\n\n    Median (IQR)\n38.30 (30.60, 66.83)\n36.70 (28.30, 51.90)\n\n\n    Range\n19.60, 130.10\n14.90, 104.40\n\n\nBMI\n\n\n\n\n    Mean ± SD\n23.02 ± 9.17\n18.32 ± 4.10\n\n\n    Median (IQR)\n18.38 (17.17, 25.66)\n17.59 (15.87, 19.48)\n\n\n    Range\n14.10, 48.07\n13.70, 42.09\n\n\nR5Hz_PP\n\n\n\n\n    Mean ± SD\n89.57 ± 25.09\n107.67 ± 32.83\n\n\n    Median (IQR)\n85.50 (72.75, 104.50)\n105.00 (85.00, 125.00)\n\n\n    Range\n43.00, 168.00\n10.00, 199.00\n\n\nR20Hz_PP\n\n\n\n\n    Mean ± SD\n77.16 ± 21.70\n88.77 ± 24.61\n\n\n    Median (IQR)\n72.00 (62.00, 88.25)\n88.00 (72.00, 102.00)\n\n\n    Range\n38.00, 135.00\n12.00, 162.00\n\n\nX5Hz_PP\n\n\n\n\n    Mean ± SD\n101.87 ± 51.52\n138.04 ± 115.20\n\n\n    Median (IQR)\n96.00 (74.75, 117.75)\n116.00 (83.00, 157.00)\n\n\n    Range\n1.69, 381.00\n-68.00, 1,117.00\n\n\nFres_PP\n\n\n\n\n    Mean ± SD\n120.55 ± 31.25\n136.75 ± 33.35\n\n\n    Median (IQR)\n111.50 (99.00, 142.50)\n132.00 (110.00, 163.00)\n\n\n    Range\n61.00, 236.00\n66.00, 225.00\n\n\n\n1 n (%)\n\n\n\n\n\n\n\n\n\n\n3.2.3 Correlations\n\n\nCode\nplot_correlation(na.omit(x), maxcat=5L)\n\n\n\n\n\n\n\n\n\nCode\ncorrelation_matrix &lt;- cor(numeric_vars)\nprint(correlation_matrix)\n\n\n                    Subject.ID Observation_number Age..months. Height..cm.\nSubject.ID          1.00000000        -0.21500257  -0.32924618 -0.17917013\nObservation_number -0.21500257         1.00000000   0.08375717  0.05602244\nAge..months.       -0.32924618         0.08375717   1.00000000  0.91367866\nHeight..cm.        -0.17917013         0.05602244   0.91367866  1.00000000\nWeight..Kg.         0.05534316        -0.04624712   0.69729363  0.74787329\nBMI                 0.16386162        -0.08167290   0.44416330  0.45814806\nR5Hz_PP            -0.26537828         0.17631576   0.40385960  0.31003693\nR20Hz_PP           -0.24942573         0.13865818   0.33200111  0.24291582\nX5Hz_PP            -0.20369376         0.09891752   0.38857148  0.37455537\nFres_PP            -0.30541160         0.11774148   0.58762865  0.52148796\n                   Weight..Kg.           BMI       R5Hz_PP    R20Hz_PP\nSubject.ID          0.05534316  1.638616e-01 -2.653783e-01 -0.24942573\nObservation_number -0.04624712 -8.167290e-02  1.763158e-01  0.13865818\nAge..months.        0.69729363  4.441633e-01  4.038596e-01  0.33200111\nHeight..cm.         0.74787329  4.581481e-01  3.100369e-01  0.24291582\nWeight..Kg.         1.00000000  9.265856e-01  1.261621e-01  0.10391395\nBMI                 0.92658562  1.000000e+00  6.430664e-06  0.01337894\nR5Hz_PP             0.12616210  6.430664e-06  1.000000e+00  0.70961067\nR20Hz_PP            0.10391395  1.337894e-02  7.096107e-01  1.00000000\nX5Hz_PP             0.13463870 -2.990486e-02  4.464806e-01  0.20055148\nFres_PP             0.19759669 -1.523383e-02  7.538067e-01  0.54826299\n                       X5Hz_PP     Fres_PP\nSubject.ID         -0.20369376 -0.30541160\nObservation_number  0.09891752  0.11774148\nAge..months.        0.38857148  0.58762865\nHeight..cm.         0.37455537  0.52148796\nWeight..Kg.         0.13463870  0.19759669\nBMI                -0.02990486 -0.01523383\nR5Hz_PP             0.44648059  0.75380665\nR20Hz_PP            0.20055148  0.54826299\nX5Hz_PP             1.00000000  0.50290470\nFres_PP             0.50290470  1.00000000\n\n\nAge (months) and Height (cm): There is a strong positive correlation (0.914) between Age (months) and Height (cm). This implies that as age increases, height tends to increase as well. This correlation is expected, as children tend to grow taller as they get older.\nWeight (Kg) and BMI: There is a strong positive correlation (0.927) between Weight (Kg) and BMI. This suggests that as weight increases, BMI (Body Mass Index) tends to increase as well. This correlation is expected because BMI is calculated using weight and height measurements.\nR5Hz_PP and Fres_PP: There is a strong positive correlation (0.754) between R5Hz_PP and Fres_PP."
  },
  {
    "objectID": "report.html#linear-mixed-modeling",
    "href": "report.html#linear-mixed-modeling",
    "title": "Report",
    "section": "3.3 Linear Mixed Modeling",
    "text": "3.3 Linear Mixed Modeling"
  },
  {
    "objectID": "report.html#model-comparisons-and-assessment",
    "href": "report.html#model-comparisons-and-assessment",
    "title": "Report",
    "section": "3.4 Model Comparisons and Assessment",
    "text": "3.4 Model Comparisons and Assessment"
  },
  {
    "objectID": "report.html#simulation-possibly",
    "href": "report.html#simulation-possibly",
    "title": "Report",
    "section": "3.5 Simulation (POSSIBLY)",
    "text": "3.5 Simulation (POSSIBLY)"
  },
  {
    "objectID": "literature.html#multimodel-inference-in-ecology-and-evolution-challenges-and-solutionsnih.gov",
    "href": "literature.html#multimodel-inference-in-ecology-and-evolution-challenges-and-solutionsnih.gov",
    "title": "Literature",
    "section": "Multimodel inference in ecology and evolution: challenges and solutions(nih.gov)",
    "text": "Multimodel inference in ecology and evolution: challenges and solutions(nih.gov)\nWhat is the goal of the paper?\nThe goal of the paper is to highlight obstacles when model averaging and using information theoretic framework and their potential solutions if they exist\nWhy is it important?\nA large number of ecologists and biologists are analyzing data with the Information theoretic framework rather than the traditional hypothesis testing. Modeling averaging becomes increasingly difficult with Linear Mixed models due to the fixed and random effects.\nHow is it solved? – methods\nThe research suggests that researchers define appropriate inputs and predictor variables and to handle collinearity with extreme care especially when dealing with the random effects of LMMs.The paper then goes on to propose strategies on model averaging and definition top model sets.\nResults/limitations, if any.\nAs the paper mentions, it is NOT an exhaustive survival of the potential problems of applying model averaging under an IT framework. Some problems still exist like determining which IT criteria to use when comparing models with random factors."
  },
  {
    "objectID": "literature.html#a-protocol-for-conducting-and-presenting-results-of-regression-type-analyses",
    "href": "literature.html#a-protocol-for-conducting-and-presenting-results-of-regression-type-analyses",
    "title": "Literature",
    "section": "A protocol for conducting and presenting results of regression-type analyses",
    "text": "A protocol for conducting and presenting results of regression-type analyses\nWhat is the goal of the paper?\nThe goal of the paper is to streamline analysis by giving the reader a 10 step protocol. It helps fellow researchers select models, justify assumptions, and validate models.\nWhy is it important?\nThis paper is great for researchers new to Linear Mixed models and are looking to use its advantages on a dataset. The protocol is extremely straightforward. Linear mixed models offer more in depth analysis and it is important that all researchers know it to further the field of ecology.\nHow is it solved? – methods\nThe paper has 10 steps with very concrete examples that include sample datasets, visualizations, and results. The reader has something tangible and can directly apply what they learned to another dataset.\nResults/limitations, if any\nThe paper is limited by showing sample results and shows no empirical data. It identifies potential pitfalls like overdispersion of fitted models but offers no real solution"
  },
  {
    "objectID": "literature.html#multimodel-inference-in-ecology-and-evolution-challenges-and-solutionsnih.gov-1",
    "href": "literature.html#multimodel-inference-in-ecology-and-evolution-challenges-and-solutionsnih.gov-1",
    "title": "Literature",
    "section": "Multimodel inference in ecology and evolution: challenges and solutions(nih.gov)",
    "text": "Multimodel inference in ecology and evolution: challenges and solutions(nih.gov)\nWhat is the goal of the paper?\nThe goal of the paper is to highlight obstacles when model averaging and using information theoretic framework and their potential solutions if they exist\nWhy is it important?\nA large number of ecologists and biologists are analyzing data with the Information theoretic framework rather than the traditional hypothesis testing. Modeling averaging becomes increasingly difficult with Linear Mixed models due to the fixed and random effects.\nHow is it solved? – methods\nThe research suggests that researchers define appropriate inputs and predictor variables and to handle collinearity with extreme care especially when dealing with the random effects of LMMs.The paper then goes on to propose strategies on model averaging and definition top model sets.\nResults/limitations, if any.\nAs the paper mentions, it is NOT an exhaustive survival of the potential problems of applying model averaging under an IT framework. Some problems still exist like determining which IT criteria to use when comparing models with random factors."
  },
  {
    "objectID": "literature.html#using-generalized-linear-mixed-models-to-evaluate-inconsistency-within-a-network-meta-analysis",
    "href": "literature.html#using-generalized-linear-mixed-models-to-evaluate-inconsistency-within-a-network-meta-analysis",
    "title": "Literature",
    "section": "Using Generalized Linear Mixed Models to Evaluate Inconsistency within a Network Meta-Analysis",
    "text": "Using Generalized Linear Mixed Models to Evaluate Inconsistency within a Network Meta-Analysis\nWhat is the goal of the paper?\nThe goal of the paper is to demonstrate how generalized linear mixed models (GLMMs) can evaluate inconsistency within network meta-analyses, improving upon traditional models by using an arm-based approach for more accurate results.\nWhy is it important?\nThe paper addresses the challenge of inconsistency between direct and indirect evidence in network meta-analysis, which can compromise the validity of conclusions, offering a more reliable framework for analysis.\nHow is it solved? – Methods\nThe authors propose an arm-based GLMM approach, which allows for flexible modeling of different outcome variables and shows improved accuracy over contrast-based methods, especially when event rates are low.\nResults/limitations, if any.\nThe arm-based model provided more accurate evaluations of design inconsistency and treatment effects compared to traditional contrast-based approaches, highlighting its utility in complex analyses involving many treatments and designs."
  },
  {
    "objectID": "literature.html#generalized-linear-mixed-model-glmm-trees-a-flexible-decision-tree-method-for-multilevel-and-longitudinal-data",
    "href": "literature.html#generalized-linear-mixed-model-glmm-trees-a-flexible-decision-tree-method-for-multilevel-and-longitudinal-data",
    "title": "Literature",
    "section": "Generalized linear mixed-model (GLMM) trees: A flexible decision-tree method for multilevel and longitudinal data",
    "text": "Generalized linear mixed-model (GLMM) trees: A flexible decision-tree method for multilevel and longitudinal data\nWhat is the goal of the paper?\nThe goal of the paper is to introduce GLMM trees, a method combining generalized linear mixed models (GLMMs) with decision trees for analyzing multilevel and longitudinal data, offering a novel approach to clinical prediction problems.\nWhy is it important?\nGLMM trees provide an interpretable and flexible method that can handle complex data structures, improving upon traditional models by simplifying the analysis and enhancing the understanding of data patterns.\nHow is it solved? – Methods\nThe paper employs GLMM trees to analyze a large dataset from UK mental health services, comparing its performance with traditional GLMMs and random forests to demonstrate its predictive accuracy and efficiency.\nResults/limitations, if any.\nThe method achieves similar predictive accuracy to traditional GLMMs and random forests but with fewer variables, showcasing its potential to streamline clinical decision-making."
  },
  {
    "objectID": "literature.html#multilevel-analysis-quantifies-variation-in-the-experimental-effect-while-optimizing-power-and-preventing-false-positives",
    "href": "literature.html#multilevel-analysis-quantifies-variation-in-the-experimental-effect-while-optimizing-power-and-preventing-false-positives",
    "title": "Literature",
    "section": "Multilevel analysis quantifies variation in the experimental effect while optimizing power and preventing false positives",
    "text": "Multilevel analysis quantifies variation in the experimental effect while optimizing power and preventing false positives\nWhat is the goal of the paper?\nThe goal of this is to show how to handle nested data in neuroscience. Oftentimes data will be collected from the same sample with different experimental conditions. The paper states that this often goes over look in neuroscience\nWhy is it important?\nNot only are several assumptions violated but experiential effects could be miscalculated leading to representing incorrect results.\nHow is it solved? – methods\nThe paper does two simulation studies to show the significance of using the appropriate significant method. Design A had cluster data that may have random effects for just the intercept.. Design B had cluster data that may have random effects in both the intercepts and experiment effect. Both cases resulted in an increase in false positive rates.\nResults/limitations, if any.\nThis data is simulated and only within the context of neuroscience data. It would be beneficial to see these results to a real world dataset"
  },
  {
    "objectID": "report.html#setting-up-a-large-dataset",
    "href": "report.html#setting-up-a-large-dataset",
    "title": "Report",
    "section": "2.1 Setting up a Large Dataset",
    "text": "2.1 Setting up a Large Dataset\nEstablishing a comprehensive dataset presented a significant challenge in training this model. Previous efforts primarily relied on three datasets—MS-COCO, Visual Genome, and YFCC100M. MS-COCO and Visual Genome, each comprising approximately 100,000 crowd-labeled images, proved insufficient. Although YFCC100M boasted over 100 million images, the authors narrowed down the selection by filtering for quality and English-language descriptions, resulting in 15 million images comparable to those in ImageNet. The abundance of publicly available data served as a primary motivation for incorporating natural language supervision. The authors curated a dataset consisting of 400 million image-text pairs, averaging around 20,000 pairs per query."
  },
  {
    "objectID": "report.html#components-of-the-clip-model",
    "href": "report.html#components-of-the-clip-model",
    "title": "Report",
    "section": "2.2 Components of the CLIP Model",
    "text": "2.2 Components of the CLIP Model\nClip as a model is a combination of a variety of subcomponents that are used to achieve this association between images and text. At its core, CLIP consists of a dual encoder system, which includes separate encoders for processing text and images. The image encoder is typically based on a convolutional neural network (CNN), capable of extracting hierarchical visual features from input images. Conversely, the text encoder leverages transformer-based models to encode textual descriptions or prompts. These encoders learn to represent images and text in a shared embedding space, where semantically similar concepts are positioned closer to each other. Additionally, CLIP incorporates a contrastive learning objective, where the model is trained to maximize the similarity between matching pairs of text and images while minimizing the similarity between non-matching pairs.\n\n2.2.1 What is a Text Encoder?\n\nFigure 3. The above figure describes the role of a text encoder. (Radford et al. 2021)\nThe text encoder within CLIP is a standard transformer encoder. A transformer can be thought of as a system which takes an entire input sequence of words, then re-presents, and compares those words to create an abstract and contextualized representation of the entire input. The self-attention mechanism within a transformer is the main mechanism that creates that contextualized representation.\n\n\n2.2.2 Transformers\nTransformers are a powerful class of deep learning models that have revolutionized natural language processing (NLP) tasks. Unlike traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs), which process data sequentially or locally, transformers leverage attention mechanisms to capture long-range dependencies and relationships within input sequences. At the heart of a transformer architecture are self-attention mechanisms, which enable the model to weigh the importance of each token in the input sequence with respect to every other token. By attending to relevant tokens and aggregating information across the entire sequence, transformers can effectively capture global context and semantic relationships, making them highly effective for tasks such as language translation, text generation, and sentiment analysis. Additionally, transformers consist of multiple layers of self-attention and feedforward neural networks, allowing them to learn hierarchical representations of input sequences.\n\n\n2.2.3 What is an Image Encoder?\n\nFigure 4. The above figure describes the role of an image encoder. (Radford et al. 2021)\nThe image encoder converts an image into a vector (a list of numbers) that represents the image’s meaning. In the CLIP paper we employ a RESNET-50 architecture as an image encoder.\n\nFigure 5. The above figure describes a simple convolution operation where we use a horizontal line detection kernel used to detect horizontal line features in the input image.\n\n\n2.2.4 What is Convolution?\nConvolutional layers, the building blocks of CNNs, operate by applying filters (also known as kernels) to input images. These filters slide across the input image, computing a dot product between the filter weights and the pixel values within each receptive field. Figure 4 above describes a simple convolution where we use a horizontal line detecting kernel to detect horizontal line features in the input image. By stacking these convolutional kernels together, we build a CNN. The following equation (eqn 1.) defines the convolution operation.\n\n\n\n2.2.5 Convolutional Neural Network\nConvolutional neural networks (CNNs) are a class of deep learning models specifically designed for processing structured grid-like data, such as images. CNNs are characterized by their ability to automatically learn hierarchical representations of visual features directly from raw pixel data. CNNs can effectively learn increasingly abstract and hierarchical representations of visual features, leading to robust performance in tasks such as image classification, object detection, and image segmentation. The whole idea behind a convolutional network is, by doing a combination of convolutions and down sampling of an image, you can extract more and more subtle feature representations which can be turned into some final output. Fig 6. describes a classic CNN architecture from a very famous object detection model YOLO (Redmon et al. 2016). In the figure each of these boxes describe an input image’s horizontal and vertical size, as well as their “depth”, in terms of number of features. The input image is an RGB image, so it has a depth of 3. By extracting more and more features, and downsampling the image via max pooling, the network distills the image into an abstract representation which is trained to hold some meaning about the image.\n\nFigure 6. A classic convolutional architecture from the YOLO paper, an landmark object detection model. (Redmon et al. 2016)\n\n\n2.2.6 Comparing the Cosine Similarity\nCosine similarity is a measure used to determine the similarity between two vectors in a high-dimensional space by computing the cosine of the angle between them. In machine learning and natural language processing, cosine similarity is valuable for comparing the similarity of documents, sentences, or words represented as vectors in a vector space model. Widely used in machine learning and natural language processing, it compares vectors’ directions rather than magnitudes, making it robust for tasks like document retrieval, text classification, and recommendation systems. The following equation 2 describes how the cosine similarity is calculated. This cosine similarity is used\n\n\nFigure 7. The figure focuses on putting the first and second part together and calculating the contrastive loss. (Radford et al. 2021)\nIn the CLIP model as seen in figure 7 cosine similarity is employed to calculate the contrastive loss by comparing the cosine similarity between text and image embeddings. This involves computing the cosine similarity between the normalized text embedding and the normalized image embedding, which helps quantify the semantic similarity between the given text and image pair. By maximizing the cosine similarity for positive pairs (matching text and image) and minimizing it for negative pairs (mismatched text and image), the model learns to effectively align semantic representations across different modalities."
  },
  {
    "objectID": "report.html#zero-shot-classification",
    "href": "report.html#zero-shot-classification",
    "title": "Report",
    "section": "2.3 Zero Shot Classification",
    "text": "2.3 Zero Shot Classification\nZero-shot classification in the CLIP model enables it to recognize objects or concepts that it hasn’t been explicitly trained on. By leveraging a large pre-trained language and vision model, CLIP learns to associate text prompts with corresponding images during training, allowing it to generalize to unseen classes at inference time. This capability is achieved through the model’s ability to understand natural language descriptions and relate them to visual content, enabling versatile and adaptive classification without the need for fine-tuning on specific classes."
  },
  {
    "objectID": "report.html#dataset",
    "href": "report.html#dataset",
    "title": "Report",
    "section": "4.1 Dataset",
    "text": "4.1 Dataset\nThe CIFAR-10 dataset is a collection of 60,000 32X32 pixel images divided into two subsets. The main subset consists of 50,000 images referred to as the training set. The secondary subset consists of 10,000 images referred to as the testing set. The images are broken into the 10 following individual classes: airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. Each class comprising 6,000 images further broken down into 5,000 in the training set and 1,000 in the testing set. CIFAR-10 is one of the most widely used data sets used to train computer vision tasks. We gained access to the dataset using the built-in Torchvision Dataset library.\nTo test the model out and try to train it on a broader dataset we also explored the caltech 256 dataset. The Caltech 256 dataset is a widely used benchmark dataset in computer vision, consisting of 30,607 images across 256 object categories. The dataset covers a diverse set of object classes, including animals, vehicles, household items, and natural scenes. This rich diversity makes the dataset challenging and suitable for evaluating the performance of computer vision algorithms, particularly in tasks such as object recognition, classification, and scene understanding. The dataset was broken down into smaller sections, where we picked up like 5000 images for training and 1000 images for validation. This dataset was available online and is free to download."
  },
  {
    "objectID": "report.html#model-training",
    "href": "report.html#model-training",
    "title": "Report",
    "section": "4.2 Model Training",
    "text": "4.2 Model Training\nThe pretrained CLIP model available on GitHub offers a selection of five different models, each varying in architecture size and complexity. For this project, we opted to utilize the “ViT-B32” model, notable for its extensive parameter count of over 155 million, making it capable of capturing rich visual and textual features during training. With our chosen model in hand, we embarked on the training process using the dataset described in the previous section.\n\nThe training loop unfolds in several key steps to iteratively refine the model’s performance:\n\nWithin each iteration, a batch of images along with their corresponding captions is loaded into memory.\nThe data is then fed through the CLIP model, which generates predictions based on the learned associations between textual and visual representations.\nThese predictions are compared against the ground truth annotations to compute the loss, which quantifies the disparity between the predicted and actual labels.\nThrough backpropagation, the computed loss is propagated back through the network, enabling the model to adjust its parameters in the direction that minimizes the loss, thereby improving its performance.\nThis fine-tuning process continues for a predetermined number of epochs, gradually refining the model’s understanding of the intricate relationship between our specific set of images and their corresponding textual descriptions.\n\nFigure 10 represents the result that we got during training where we have plotted validation accuracy and loss vs no. of epochs.\n\nFigure 10. The above plot represents the loss and validation accuracy vs no. of epochs.\nAs you can see from the plot there is a lot more data and a lot more many epochs required in addition with some tunning to get the loss to be zero and increase the accuracy. Due to equipment limitation we had to run a smaller subset of the data for a lesser number of epochs due to the shear size of the model. The model contains over 155 million parameters and the huge dataset posed a huge challenge for our personal computers.\nSee the Code tab for a full code listing."
  },
  {
    "objectID": "report.html#classification-examples",
    "href": "report.html#classification-examples",
    "title": "Report",
    "section": "4.3 Classification Examples",
    "text": "4.3 Classification Examples\n\n4.3.1 Principle of Similarity Matrix\nIn the context of the CLIP model, a similarity matrix serves as a vital tool for quantifying the semantic relationships between textual descriptions and associated images. Represented as a square matrix, it captures pairwise similarities between elements in the dataset, aiding tasks such as image retrieval and clustering. By computing similarity scores between image-caption pairs, the matrix facilitates the exploration of underlying patterns and structures within multimodal data. This matrix plays a crucial role in enhancing our understanding of the intricate connections between textual and visual information, driving advancements in multimodal learning within the CLIP framework.\n\nFigure 11. The above figure describes a similarity matrix. And how the clip model is structured to find the similarity between text and images. (Radford et al. 2021)\n\n\n4.3.2. Cosine Similarity Between Text and Image Features\nIn the illustration below, we present a straightforward classification example for the CLIP model. The image depicts a cosine similarity matrix comparing five images with five distinct captions. The diagonal of the matrix represents the similarity index, showcasing how each image aligns with its corresponding caption. This visualization offers a clear depiction of the model’s ability to discern semantic relationships between images and textual descriptions through cosine similarity computations.\n\nFigure 12. The above image illustrates the working of the CLIP model where we see the highest similarity between images and respective captions. (Radford et al. 2021)"
  },
  {
    "objectID": "report.html#possible-real-world-applications",
    "href": "report.html#possible-real-world-applications",
    "title": "Report",
    "section": "4.4 Possible real world applications",
    "text": "4.4 Possible real world applications\n\n4.4.1 Robotics Application\nIn today’s rapidly advancing world, there is a growing demand for robots designed to perform intricate tasks aimed at enhancing our daily lives. Among the recent trends, there’s a notable surge in the development of autonomous robots. In this context, we advocate for the integration of vision and language-based models into the field of robotics. These models enable robots to leverage camera input, identify various objects within their view, and generate tailored prompts for user interaction.\nFor instance, imagine a humanoid robot equipped with vision capabilities observing objects on a table. The robot can then provide users with a range of options, such as ‘Pick up the bottle’ or ‘Dispose of the trashcan.’ This approach empowers users to receive prompts that outline the robot’s potential actions, allowing us select the most appropriate response.\n\nFigure 13. Humanoid Robotics Application. The image on the right shows the prompts that the user will see as possible actions.\n\n\n4.4.2 Healthcare Application\nThere are numerous examples throughout the healthcare field where deep learning models such as CLIP may open the door to multimodal data integration which could enhance our understanding of individual health (i.e., personalized medicine) as well as population-level epidemiology (ex: pandemic surveillance). A recent article by Acosta Et. Al. has highlighted 7 opportunities for general multimodal biomedical AI as shown in Figure 14 (Acosta et al. 2022). \n\nFigure 14. Data modalities and their applications in healthcare (Acosta et al. 2022). CLIP specifically has applications with EHR data that is uploaded as images, such as radiological scans and environmental monitoring.\nHealthcare would specifically benefit from the CLIP model where images and scans are utilized, which is common in specialities such as oncology, neurology, radiology, and orthopedics. Some CNN models are already being implemented to separate “noise’ from the true image that is produced by scans in these specialities, as shown in Figure 15 (Liu et al. 2021). As explained by Acosta Et. Al., models like CLIP could play a crucial role in not just separating noise, but also in extracting and interpreting information from scans and EHR data that are uploaded as images, which is common in Electronic Health Record (EHR) systems (Acosta et al. 2022). Taking this a step further, once analyzed by CLIP, this data could then be integrated into the existing collection of personalized omics data that is maintained in EHRs, providing a much more robust assessment of individual health. \n\nFigure 15. An example of radiological “cleaning” with CNNs (Liu et al. 2021). Low-resolution and low-count PET scans of the abdomen and brain were cleaned with a trained CNN to produce a deblurred or denoised image that resembled the full-count and high-resolution images taken on a higher quality and more expensive machine (Liu et al. 2021).\nEnvironmental monitoring is an area of medicine that is frequently overlooked but could potentially benefit from models like CLIP. Acosta Et. Al. briefly mention that these models could be used in conjunction with ambient sensors to improve remote care systems both at home and in assisted living facilities (ALFs) (Acosta et al. 2022). The CLIP model could be trained to recognize high-risk situations, such as stroke symptoms (which include physical changes) and accidents such as falls. CLIP may even be able to recognize activities of mental stimulation compared to boredom, which would benefit the goals of ALFs to provide physical as well as mental support to the aging population. \n\n\n4.4.3 Assistive Device Application\nAnother real-world application of the CLIP model is to employ the model as an assistive technology to aid people with visual impairments. Historically, the majority of blind people have had to rely on passive devices such as a walking cane to navigate their surroundings using the cane to detect objects in their path. A small number utilize guide dogs to navigate around obstacles. But these methods are not ideal. As an example, a guide dog is unable to interpret street signs. We imagine a CLIP model being combined with a large language model deployed to a device that can recognize & identify objects for the blind and for people with visual impairments. The advantage of the CLIP model is that the output object identification won’t be limited to a one-word response. Leveraging the fact that model is trained on captions and textual descriptions. The output response has the potential to be able to provide a vivid sentence describing the object as it relates to its surroundings. This would allow the visually impaired person to essentially have awareness of their environment."
  },
  {
    "objectID": "report.html#limitations",
    "href": "report.html#limitations",
    "title": "Report",
    "section": "5.1 Limitations",
    "text": "5.1 Limitations\nCLIP is geared towards a contrastive objective rather than a prediction objective. While this approach requires relatively significant less computational resources it does it at a cost to accuracy. A contrastive objective tends to be better at predicting the relationship between two objects, in this case being the text and the image instead of predicting it. As such the model tends to have difficulty distinguishing between objects of a specific class.  As an example, being able to correctly label different breeds of dogs, rather having more success in just being able to classify the imagine as a dog. CLIP is a generalized model meaning that it was created to be able to recognize a wide variety of tasks by taking images and relating them to text to improve zero-shot performance. However, that generality underperforms on several specialized tasks such as medical imaging, self-driving tasks, and satellite image classification to name a few (Radford et al. 2021).\nCLIP is a new model having been introduced in 2021 and naturally the CLIP model has yet to reach the zero-shot performance accuracy of the SOTA models available today. The accuracy margin increases even more when compared to specialty models trained for a specific task as an example celebrity identification. Achieving such accuracy would require an additional 1000 times increase in the computation resources which is impossible given current hardware limitations (Radford et al. 2021). GPU, memory requirements, and energy consumption being at the forefront of those limitations given the use of today’s hardware. The hardware limitations could be one of the reasons why the researchers at OpenAI used a contrastive approach mentioned in the previous paragraph."
  },
  {
    "objectID": "report.html#conclusion",
    "href": "report.html#conclusion",
    "title": "Report",
    "section": "5.2 Conclusion",
    "text": "5.2 Conclusion\nRegardless of these limitations the CLIP model advances image classification by combining images and text to learn. It accomplishes this by its pre-training task which harnesses image-text pairs which are jointly trained on an image encoder in addition to a text encoder. CLIP shows great promise in its efficiency in relation to its performance. By contrast the models presented in the paper were trained on a few datasets, with the largest Vision Transformer taking 12 days to train on only 592 V100 GPUs and the largest Vision Transformer taking 18 days to train on 256 V100 GPUs (Radford et al. 2021). Compare that with some of the larger image classification models taking months and some even more than a year to train. \nFurthermore, the CLIP model is scalable given the number of captions enabled images that can be found on the internet on websites such as social media sites making it ideal for self-supervised learning. Self-supervised in the sense that the training data can be scrapped directly from the internet instead of relying on people or machines to annotate and label the data. This step alone significantly reduces the pre-training time. Just as it took about a decade for the leap from LeNet to Alexnet to materialize in part due to the hardware limitations, mostly the storage capacity of computers and the advancement of the GPU.  We predict that with continuing research and the advancement of the next generation of hardware, the CLIP model will play a pivotal role in the future of classification problems."
  }
]